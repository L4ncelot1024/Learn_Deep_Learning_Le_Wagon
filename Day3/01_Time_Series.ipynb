{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01-Time-Series.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L4ncelot1024/Learn_Deep_Learning_Le_Wagon/blob/main/Day3/01_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa49bUnKyRgF"
      },
      "source": [
        "# Time series forecasting: Weather Forceast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU8C5qm_4vZb"
      },
      "source": [
        "This tutorial is an introduction to time series forecasting using Recurrent Neural Networks (RNNs). This is covered in two parts: first, you will forecast a univariate time series, then you will forecast a multivariate time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsfJvoe4MKBf"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# Force the tensorflow version to be 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rZnJaGTWQw0"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TokBlnUhWFw9"
      },
      "source": [
        "## Data\n",
        "This tutorial uses a [weather time series dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the [Max-Planck-Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/index.php/Main/HomePage).\n",
        "\n",
        "This dataset contains __14__ different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by Fran√ßois Chollet for his book [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyv_i85IWInT"
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX6uGeeeWIkG"
      },
      "source": [
        "df = pd.read_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdbOWXiTWM2T"
      },
      "source": [
        "Let's take a glance at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojHE-iCCWIhz"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfbpcV0MWQzl"
      },
      "source": [
        "Questions about the data:\n",
        "\n",
        "- How often do we record the measures?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQEvS-gLMuFc"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "As you can see above, an observation is recorded __every 10 minutes__. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrfBG_7yMuCe"
      },
      "source": [
        "- If we want to predict the temperature 6 hours in the future, and we choose 5 days of measures. How many observations should we have in one training input?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmLne-cHMt_M"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "In order to make this prediction, you would create a window containing the last 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEo531_UlGM3"
      },
      "source": [
        "### Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLGHlSYUMt8U"
      },
      "source": [
        "The function below returns the above described windows of time for the model to train on.\n",
        "\n",
        "- The parameter `history_size` is the number of time steps of the past window of information to use.\n",
        "\n",
        "- The `target_size` is how far in the future (= number of time steps) does the model need to learn to predict. The `target_size` is the label that needs to be predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AoxQuTrWIbi"
      },
      "source": [
        "def extract_data_labels(dataset, start_index, end_index, history_size, target_size):\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i)\n",
        "    # Reshape data from (history_size,) to (history_size, 1)\n",
        "    data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
        "    labels.append(dataset[i+target_size])\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoFJZmXBaxCc"
      },
      "source": [
        "Here we're splittting our data in train & test with a temporal split since we want to be good at predicting the future. \n",
        "\n",
        "So the first 300,000 rows of the data will be the training dataset, and the remaining part will be the validation dataset. This amounts to ~2100 days worth of training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ia-MPAHxbInX"
      },
      "source": [
        "TRAIN_SPLIT = 300000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EowWDtaNnH1y"
      },
      "source": [
        "We set the seed for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x-GgENynHdx"
      },
      "source": [
        "tf.random.set_seed(13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YEwr-NoWUpV"
      },
      "source": [
        "## Part 1: Forecast a univariate time series\n",
        "First, you will train a model using only a single feature (temperature), and use it to make predictions for that value in the future.\n",
        "\n",
        "Let's first extract only the temperature from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbdcnm1_WIY9"
      },
      "source": [
        "# Here we extract our univariate data, and we set the time as index to keep the order\n",
        "uni_data = df['T (degC)']\n",
        "uni_data.index = df['Date Time']\n",
        "uni_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQB-46MyWZMm"
      },
      "source": [
        "Let's observe how this data looks across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftOExwAqWXSU"
      },
      "source": [
        "uni_data.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jlAKoSYliK5"
      },
      "source": [
        "### Standardisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSEiDqBWXQa"
      },
      "source": [
        "# We convert the data into a np.ndarray\n",
        "uni_data = uni_data.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eFckdUUHWmT"
      },
      "source": [
        "It is important to normalize features before training a neural network. A common way to do so is by subtracting the mean and dividing by the standard deviation of each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZmxf7x4l_4Y"
      },
      "source": [
        "# TODO: normalize your input features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eji6njXvHusN"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "uni_data_train = uni_data[:TRAIN_SPLIT]\n",
        "scaler.fit(uni_data_train[:, np.newaxis])\n",
        "\n",
        "uni_data_scaled = scaler.transform(uni_data[:, np.newaxis])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxtM9OBcmcUC"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "uni_data_train = uni_data[:TRAIN_SPLIT]\n",
        "scaler.fit(uni_data_train[:, np.newaxis])\n",
        "\n",
        "uni_data_scaled = scaler.transform(uni_data[:, np.newaxis])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn8A_nrccKtn"
      },
      "source": [
        "Let's now create the data for the univariate model. First, we want to predict the next time step given an history size of 20 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJJ-T49vWXOZ"
      },
      "source": [
        "univariate_past_history = 20\n",
        "univariate_future_target = 0\n",
        "\n",
        "x_train_uni, y_train_uni = extract_data_labels(uni_data_scaled, 0, TRAIN_SPLIT,\n",
        "                                           univariate_past_history,\n",
        "                                           univariate_future_target)\n",
        "x_val_uni, y_val_uni = extract_data_labels(uni_data_scaled, TRAIN_SPLIT, None,\n",
        "                                       univariate_past_history,\n",
        "                                       univariate_future_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWpVMENsdp0N"
      },
      "source": [
        "This is what the `univariate_data` function returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feDd95XFdz5H"
      },
      "source": [
        "print ('Single window of past history')\n",
        "print (x_train_uni[0])\n",
        "print ('\\n Target temperature to predict')\n",
        "print (y_train_uni[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hni3Jt9OMR1_"
      },
      "source": [
        "Now that the data has been created, let's take a look at a single example. The information given to the network is given in blue, and it must predict the value at the red cross."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVukM9dRipop"
      },
      "source": [
        "def create_time_steps(length):\n",
        "  time_steps = []\n",
        "  for i in range(-length, 0, 1):\n",
        "    time_steps.append(i)\n",
        "  return time_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQeGvh7cWXMR"
      },
      "source": [
        "def show_plot(plot_data, names, delta , title):\n",
        "  labels = ['History', 'True Future'] + names\n",
        "  m = len(plot_data)\n",
        "  marker = ['.-', 'rx']\n",
        "  for i in range(m-2):\n",
        "    marker.append('o')\n",
        "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "  if delta:\n",
        "    future = delta\n",
        "  else:\n",
        "    future = 0\n",
        "\n",
        "  plt.title(title)\n",
        "  for i, x in enumerate(plot_data):\n",
        "    if i:\n",
        "      plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "               label=labels[i])\n",
        "    else:\n",
        "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "  plt.legend()\n",
        "  plt.xlim([time_steps[0], (future+5)*2])\n",
        "  plt.xlabel('Time-Step')\n",
        "  return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd05iV-UWXKL"
      },
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0]], [], 0, 'Sample Example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5rUJ_2YMWzG"
      },
      "source": [
        "### Baseline\n",
        "Before proceeding to train a model, let's first set a simple baseline. Given an input point, the baseline method looks at all the history and predicts the next point to be the average of the last 20 observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9nYWcxMMWnr"
      },
      "source": [
        "# TODO: define a baseline which simply compute the mean of the history\n",
        "# [EXTRA]: if you have other ideas for a simple baseline, you can add them there\n",
        "def baseline(history):\n",
        "  pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGLROI8vnrNt"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "def baseline(history):\n",
        "  return np.mean(history)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMcdFYKQMWlm"
      },
      "source": [
        "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], ['Baseline Prediction'], 0,\n",
        "           'Baseline Prediction Example')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "067m6t8cMakb"
      },
      "source": [
        "Let's see if you can beat this baseline using a recurrent neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4crpOcoMlSe"
      },
      "source": [
        "### Recurrent neural network\n",
        "\n",
        "Recall:\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of neural network well-suited to time series data. RNNs process a time series step-by-step, maintaining an internal state summarizing the information they've seen so far.\n",
        "\n",
        "Here, you will use a specialized RNN layer called Long Short Term Memory ([LSTM](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw3x_39Zn4Rk"
      },
      "source": [
        "x_train_uni.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDbpHosCMWZO"
      },
      "source": [
        "# TODO: create a Network with 1 LSTM hidden layer and compile it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDQgiNiPoHKz"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Which loss should you use?\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "simple_lstm_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "simple_lstm_model.compile(optimizer='adam', loss='mae')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOGZtDAqMtSi"
      },
      "source": [
        "Let's make a sample prediction, to check the output of the model and make sure it goes through."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxeTOxv6obOj"
      },
      "source": [
        "# TODO: predict on one random sample of the validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK_JGLv-ptMh"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "print(simple_lstm_model.predict(x_val_uni[:1]).shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYz6RN_mMyau"
      },
      "source": [
        "Let's train the model now. Due to the large size of the dataset, in the interest of saving time, each epoch will only run for 200 steps, instead of the complete training data as normally done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0opH9xi5MtIk"
      },
      "source": [
        "# TODO: fit your model here using the following settings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPccDllsp2Na"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "simple_lstm_model.fit(x_train_uni, y_train_uni, batch_size=BATCH_SIZE,\n",
        "                      epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      shuffle=True,\n",
        "                      validation_data=(x_val_uni, y_val_uni), validation_steps=50)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1OKHda5pRMg"
      },
      "source": [
        "history_df = pd.DataFrame(simple_lstm_model.history.history)\n",
        "history_df['epochs'] = history_df.index\n",
        "history_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dggkFiAkpRJZ"
      },
      "source": [
        "# TODO: plot the loss and metrics curv on both the train and validation set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jumGGtCPvFhZ"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        " \n",
        "for i, metric in enumerate(['loss', 'mse']):\n",
        "  ax = axes[i]\n",
        "  history_df.plot('epochs', f'{metric}', color='g', label='train', ax=ax)\n",
        "  history_df.plot('epochs', f'val_{metric}', color='r', label='val', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euyPo_lyNryZ"
      },
      "source": [
        "#### Predict using the simple LSTM model\n",
        "Now that you have trained your simple LSTM, let's try and make a few predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlF_Vik6qyZf"
      },
      "source": [
        "x_val_uni.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2rRLrs8MtGU"
      },
      "source": [
        "# Here we plot the predictions using an LSTM and the Baseline on 3 sample\n",
        "ids = np.random.randint(0, len(x_val_uni), 3)\n",
        "for i in ids:\n",
        "  x, y = x_val_uni[i], y_val_uni[i]\n",
        "  plot = show_plot([x, y, baseline(x),\n",
        "                    simple_lstm_model.predict(x[np.newaxis, :])], ['Baseline Prediction', 'LSTM Prediction'],\n",
        "                   0, 'Simple LSTM model')\n",
        "  plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-AVEJyRNvt0"
      },
      "source": [
        "This looks better than the baseline. Now that you have seen the basics, let's move on to part two, where you will work with a multivariate time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlJYi3_HXcw8"
      },
      "source": [
        "## Part 2: Forecast a multivariate time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxNZ2GM7DPm"
      },
      "source": [
        "The original dataset contains 14 features. For simplicity, this section considers only three of the original fourteen. The features used are air temperature, atmospheric pressure, and air density. \n",
        "\n",
        "To use more features, add their names to this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DphrB7bxSNDd"
      },
      "source": [
        "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQUSiJfUpXJ"
      },
      "source": [
        "features = df[features_considered]\n",
        "features.index = df['Date Time']\n",
        "features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSfhTZi5r15R"
      },
      "source": [
        "Let's have a look at how each of these features vary across time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdgC8zvGr21X"
      },
      "source": [
        "features.plot(subplots=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzueXNYttJOI"
      },
      "source": [
        "### Standardisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqStgZ-O1b3_"
      },
      "source": [
        "As mentioned, the first step will be to normalize the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7VuNIwfHRHx"
      },
      "source": [
        "# TODO: normalize your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAdj69LhvLxZ"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "dataset = features.values\n",
        "dataset_train = dataset[:TRAIN_SPLIT]\n",
        "scaler.fit(dataset_train)\n",
        "\n",
        "dataset_scaled = scaler.transform(dataset_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyuGuJUgjUK3"
      },
      "source": [
        "### Single step model\n",
        "In a single step setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as below, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-rVX4d3OF86"
      },
      "source": [
        "def extract_data_labels_multi(dataset, target, start_index, end_index, history_size,\n",
        "                      target_size, step, single_step=False):\n",
        "  '''\n",
        "  Extract features and label from the dataset, sampling the data in the index\n",
        "  range (start_index, end_index), with a step size of step.\n",
        "  It uses a number of time steps defined by history_size for the features and\n",
        "  return information after target_size timesteps for the label.\n",
        "  '''\n",
        "  data = []\n",
        "  labels = []\n",
        "\n",
        "  start_index = start_index + history_size\n",
        "  if end_index is None:\n",
        "    end_index = len(dataset) - target_size\n",
        "\n",
        "  for i in range(start_index, end_index):\n",
        "    indices = range(i-history_size, i, step)\n",
        "    data.append(dataset[indices])\n",
        "\n",
        "    if single_step:\n",
        "      labels.append(target[i+target_size])\n",
        "    else:\n",
        "      labels.append(target[i:i+target_size])\n",
        "\n",
        "  return np.array(data), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWVGYwbN2ITI"
      },
      "source": [
        "In this tutorial, the network is shown data from the last five (5) days, i.e. 720 observations that are sampled every hour. The sampling is done every one hour since a drastic change is not expected within 60 minutes. Thus, 120 observation represent history of the last five days.  For the single step prediction model, the label for a datapoint is the temperature 12 hours into the future. In order to create a label for this, the temperature after 72(12*6) observations is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlhVGzPhmMYI"
      },
      "source": [
        "past_history = 720\n",
        "future_target = 72\n",
        "STEP = 6\n",
        "\n",
        "x_train_single, y_train_single = extract_data_labels_multi(dataset_scaled, dataset_scaled[:, 1], 0,\n",
        "                                                   TRAIN_SPLIT, past_history,\n",
        "                                                   future_target, STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = extract_data_labels_multi(dataset_scaled, dataset_scaled[:, 1],\n",
        "                                               TRAIN_SPLIT, None, past_history,\n",
        "                                               future_target, STEP,\n",
        "                                               single_step=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7maaugd1EOB-"
      },
      "source": [
        "x_train_single.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBqFOCKiE6gg"
      },
      "source": [
        "y_train_single.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CamMObrwPhnp"
      },
      "source": [
        "Let's look at a single data-point.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tVKm-ZIPls0"
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aWec9_nlxBl"
      },
      "source": [
        "# TODO: build a Keras NN with one hidden layer of LSTM for the single step task"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJ9m2A-vX8x"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "single_step_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(32, input_shape=x_train_single.shape[-2:]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae', metrics=['mse'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYhUfWjwOPFN"
      },
      "source": [
        "Let's check out a sample prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yY7FodHVOPsH"
      },
      "source": [
        "# TODO: check that your model can predict on an input sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv_GDJ6dvSyK"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "print(single_step_model.predict(x_val_single[:1]).shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0jnt2l2mwkl"
      },
      "source": [
        "# TODO: Fit your model with the same settings as previously\n",
        "single_step_history = single_step_model.fit(x_train_single, y_train_single,\n",
        "                                            epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            shuffle=True,\n",
        "                                            validation_data=(x_val_single, y_val_single),\n",
        "                                            validation_steps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lBKA-z5yYV"
      },
      "source": [
        "# TODO: plot metrics of train and val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9e-ouWJ3pdc"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "history_df = pd.DataFrame(single_step_history.history)\n",
        "history_df['epochs'] = history_df.index\n",
        "history_df\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        " \n",
        "for i, metric in enumerate(['loss', 'mse']):\n",
        "  ax = axes[i]\n",
        "  history_df.plot('epochs', f'{metric}', color='g', label='train', ax=ax)\n",
        "  history_df.plot('epochs', f'val_{metric}', color='r', label='val', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfjrGAlEUp7i"
      },
      "source": [
        "#### Predict a single step future\n",
        "Now that the model is trained, let's make a few sample predictions. The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyU5YUpwL5Z"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1qmPLLVUpuN"
      },
      "source": [
        "ids = np.random.randint(0, len(x_val_single), 3)\n",
        "for i in ids:\n",
        "  x, y = x_val_single[i], y_val_single[i]\n",
        "  plot = show_plot([x[:, 1], y, single_step_model.predict(x[np.newaxis, :])],\n",
        "                   ['Single Step LSTM'],\n",
        "                    12,\n",
        "                   'Single Step Prediction')\n",
        "  plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5lZqYUkxcc-"
      },
      "source": [
        "# [EXTRA]: Include more features in your model and/or tune the architecture to improve your predictions !"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GnE087bJYSu"
      },
      "source": [
        "### Multi-Step model\n",
        "In a multi-step prediction model, given a past history, the model needs to learn to predict a range of future values. Thus, unlike a single step model, where only a single future point is predicted, a multi-step model predict a sequence of the future.\n",
        "\n",
        "For the multi-step model, the training data again consists of recordings __over the past 5 days__ sampled __every hour__. However, here, the model needs to learn to predict the temperature __for the next 12 hours__. Since an obversation is taken every 10 minutes, the output is __72 predictions__. For this task, the dataset needs to be prepared accordingly, thus the first step is just to create it again, but with a different target window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZCk9fqyJZqX"
      },
      "source": [
        "# TODO: create the train and val dataset with this new setting\n",
        "# (use the extract_data_labels_multi function defined before)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYRPuIUtyFJe"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "future_target = 72\n",
        "x_train_multi, y_train_multi = extract_data_labels_multi(dataset, dataset[:, 1], 0,\n",
        "                                                 TRAIN_SPLIT, past_history,\n",
        "                                                 future_target, STEP)\n",
        "x_val_multi, y_val_multi = extract_data_labels_multi(dataset, dataset[:, 1],\n",
        "                                             TRAIN_SPLIT, None, past_history,\n",
        "                                             future_target, STEP)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImXPwAGRtWy"
      },
      "source": [
        "Let's check out a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpWDcBkQRwS-"
      },
      "source": [
        "print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
        "print ('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZcg8FWpSG8K"
      },
      "source": [
        "Plotting a sample data-point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksXKVbwBV7D3"
      },
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCQKetflZRMF"
      },
      "source": [
        "In this plot and subsequent similar plots, the history and the future data are sampled every hour."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6G8bacQR4w2"
      },
      "source": [
        "multi_step_plot(x_train_multi[0], y_train_multi[0], np.array([0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOjz8DzZ4HFS"
      },
      "source": [
        "Since the task here is a bit more complicated than the previous task, the model now consists of two LSTM layers. Finally, since 72 predictions are made, the dense layer outputs 72 predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoBi860EGW6H"
      },
      "source": [
        "x_train_single.shape[-2:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byAl0NKSNBP6"
      },
      "source": [
        "# TODO: build and compile the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glhhNtrvCnQ_"
      },
      "source": [
        "multi_step_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuaHzYtzbBv"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "multi_step_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=x_train_multi.shape[-2:]),\n",
        "    tf.keras.layers.LSTM(16, activation='relu'),\n",
        "    tf.keras.layers.Dense(72)\n",
        "])\n",
        "\n",
        "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae', metrics=['mse'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvB7zBqVSMyl"
      },
      "source": [
        "Let's see how the model predicts before it trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13_ZWvB9SRlZ"
      },
      "source": [
        "# TODO: check your model is able to predict on one sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9gbPBihzj_3"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "print(multi_step_model.predict(x_train_multi[:1]).shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uwOhXo3Oems"
      },
      "source": [
        "# TODO: fit your model with the same settins as before\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_00ltG_0KCK"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "multi_step_history = multi_step_model.fit(x_train_multi, y_train_multi,\n",
        "                                          epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                                          steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                          shuffle=True,\n",
        "                                          validation_data=(x_val_multi, y_val_multi),\n",
        "                                          validation_steps=50)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfQoBjQ5l7U"
      },
      "source": [
        "history_df = pd.DataFrame(multi_step_history.history)\n",
        "history_df['epochs'] = history_df.index\n",
        "history_df\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        " \n",
        "for i, metric in enumerate(['loss', 'mse']):\n",
        "  ax = axes[i]\n",
        "  history_df.plot('epochs', f'{metric}', color='g', label='train', ax=ax)\n",
        "  history_df.plot('epochs', f'val_{metric}', color='r', label='val', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQXboPW313M"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "history_df = pd.DataFrame(single_step_history.history)\n",
        "history_df['epochs'] = history_df.index\n",
        "history_df\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        " \n",
        "for i, metric in enumerate(['loss', 'mse']):\n",
        "  ax = axes[i]\n",
        "  history_df.plot('epochs', f'{metric}', color='g', label='train', ax=ax)\n",
        "  history_df.plot('epochs', f'val_{metric}', color='r', label='val', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDg94-yq4pas"
      },
      "source": [
        "#### Predict a multi-step future\n",
        "Let's now have a look at how well your network has learnt to predict the future."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mj6mIAN4Tv9"
      },
      "source": [
        "multi_step_model.predict(x[np.newaxis, :]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt22wq6fyIBU"
      },
      "source": [
        "ids = np.random.randint(0, len(x_val_multi), 3)\n",
        "for i in ids:\n",
        "  x, y = x_val_multi[i], y_val_multi[i]\n",
        "  multi_step_plot(x, y, multi_step_model.predict(x[np.newaxis, :])[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf97Q7hVLiOK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
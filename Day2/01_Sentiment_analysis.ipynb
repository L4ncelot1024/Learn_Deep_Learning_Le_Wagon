{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L4ncelot1024/Learn_Deep_Learning_Le_Wagon/blob/main/Day2/01_Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OO06xLD1oRz"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuW66mp_1oSB"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this exercise, you will do text classification starting from raw text (as\n",
        "a set of text files on disk). You will apply this workflow on the IMDB sentiment\n",
        "classification dataset (unprocessed version) where the goal is to say if a review is positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYWNNK2W1oSC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZCKZoke1oSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0adaa62-916e-434b-83c7-68f2d565a0d3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK6D2DVpvAsm"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBIDWCBP8pvM"
      },
      "source": [
        "This first part is a good example of the beginning of any Deep Learning project: first we need to look at the data and prepare them ! \n",
        "\n",
        "Our data here are movie reviews written by users on the [imdb website](https://www.imdb.com/). We download them already labelled, the positive and negative ones are split in two different folders. \n",
        "\n",
        "But the data are not processed yet so we will use tensorflow to prepare the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPm5yxQY1oSD"
      },
      "source": [
        "### Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsZMtVfD1oSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9b390f-417d-4b26-c36a-bbccc4a63941"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  25.5M      0  0:00:03  0:00:03 --:--:-- 25.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A76ylcK61oSE"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder. The following bash commands prints the content of the different folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA_Q09Is1oSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aafa840-f5d2-4657-b2cc-571d7939862a"
      },
      "source": [
        "!ls aclImdb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDLe8q1KhPTz",
        "outputId": "cf3ec754-9d78-41b2-9751-33b0cbf1bf58"
      },
      "source": [
        "!cat aclImdb/README"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYxjwbnN1oSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40ebbdc-969d-4fd7-f3a3-17bd0a6cfdcb"
      },
      "source": [
        "!ls aclImdb/test"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD2ARRCm1oSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8259e8fb-1d9a-44d3-c4b4-865c45e5f1eb"
      },
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh4osQqq1oSG"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each file represents one review (**positive** if in the *pos* folder or **negative** if in the *neg* folder). The following command display one example from each folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pQW3Phy1oSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8eb9f30-ece7-4f1d-9b0b-90073791ac8c"
      },
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv2W-0Ur9sq8",
        "outputId": "c123e7ec-f896-4f1f-bf68-05873acd65e6"
      },
      "source": [
        "!cat aclImdb/test/neg/62_2.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Humm, an Italian movie starred by David hasselhoff and Linda Blair, I wasn´t expecting very much, to be honest and in fact, I took even less than I was expecting. It doesn´t mean this movie is the worst I have seen because I have watched worse things than this but the plot was most of the times confusing and uninteresting and some good gore scenes are the only thing saving this. Apart from that you are going to love some special effects, they are really cheesy and bad. Now I only want to watch \"Troll 3\" by this same director, sure it is not going to be worse than that."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6t56qxl1oSH"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fe2rAFX1oSH"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cx54EN1oSH"
      },
      "source": [
        "Now we would like to load our data from the files into the memory so we can feed them into our model. \n",
        "\n",
        "To do that, use the utility `tf.keras.preprocessing.text_dataset_from_directory` to\n",
        "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed into class-specific folders. \n",
        "\n",
        "We will also split the data since this function already contains the logic to split a dataset into validation and test (using the two argument `validation_split` and `subset`).\n",
        "\n",
        "Your goal here is to use this function to generate the training, validation, and test datasets. \n",
        "\n",
        "The validation and training datasets are generated from two subsets of the `train` directory, with the following ratio: 80% in the training and 20% in the validation. The test dataset comes directly from the test folder. \n",
        "\n",
        "You can use a reasonable batch size (for instance `32`). \n",
        "\n",
        "Finishes with a print of the sizes of your 3 sets. \n",
        "\n",
        "NB: When using the `validation_split` & `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation & training splits you get have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MFAtFrB3i49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d6f7b8-206f-4e72-f8b5-d4fda9d369f1"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train',batch_size=32,shuffle=True,seed=42,validation_split=0.2,subset='training')\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/train',batch_size=32,shuffle=True,seed=42,validation_split=0.2,subset='validation')\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory('aclImdb/test',batch_size=32,shuffle=True,seed=42)\n",
        "print(len(raw_train_ds),len(raw_val_ds),len(raw_test_ds))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "625 157 782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlwLy-P_3_8v"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Number of batches in raw_train_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_train_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_test_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_test_ds)\n",
        ")\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuUqlAKk1oSI"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLxw0HlU1oSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09b868d-68dd-44b3-e774-cb31c4762c04"
      },
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'\"Pandemonium\" is a horror movie spoof that comes off more stupid than funny. Believe me when I tell you, I love comedies. Especially comedy spoofs. \"Airplane\", \"The Naked Gun\" trilogy, \"Blazing Saddles\", \"High Anxiety\", and \"Spaceballs\" are some of my favorite comedies that spoof a particular genre. \"Pandemonium\" is not up there with those films. Most of the scenes in this movie had me sitting there in stunned silence because the movie wasn\\'t all that funny. There are a few laughs in the film, but when you watch a comedy, you expect to laugh a lot more than a few times and that\\'s all this film has going for it. Geez, \"Scream\" had more laughs than this film and that was more of a horror film. How bizarre is that?<br /><br />*1/2 (out of four)'\n",
            "0\n",
            "b\"David Mamet is a very interesting and a very un-equal director. His first movie 'House of Games' was the one I liked best, and it set a series of films with characters whose perspective of life changes as they get into complicated situations, and so does the perspective of the viewer.<br /><br />So is 'Homicide' which from the title tries to set the mind of the viewer to the usual crime drama. The principal characters are two cops, one Jewish and one Irish who deal with a racially charged area. The murder of an old Jewish shop owner who proves to be an ancient veteran of the Israeli Independence war triggers the Jewish identity in the mind and heart of the Jewish detective.<br /><br />This is were the flaws of the film are the more obvious. The process of awakening is theatrical and hard to believe, the group of Jewish militants is operatic, and the way the detective eventually walks to the final violent confrontation is pathetic. The end of the film itself is Mamet-like smart, but disappoints from a human emotional perspective.<br /><br />Joe Mantegna and William Macy give strong performances, but the flaws of the story are too evident to be easily compensated.\"\n",
            "0\n",
            "b'Great documentary about the lives of NY firefighters during the worst terrorist attack of all time.. That reason alone is why this should be a must see collectors item.. What shocked me was not only the attacks, but the\"High Fat Diet\" and physical appearance of some of these firefighters. I think a lot of Doctors would agree with me that,in the physical shape they were in, some of these firefighters would NOT of made it to the 79th floor carrying over 60 lbs of gear. Having said that i now have a greater respect for firefighters and i realize becoming a firefighter is a life altering job. The French have a history of making great documentary\\'s and that is what this is, a Great Documentary.....'\n",
            "1\n",
            "b\"It's boggles the mind how this movie was nominated for seven Oscars and won one. Not because it's abysmal or because given the collective credentials of the creative team behind it really ought to deserve them but because in every category it was nominated Prizzi's Honor disappoints. Some would argue that old Hollywood pioneer John Huston had lost it by this point in his career but I don't buy it. Only the previous year he signed the superb UNDER THE VOLCANO, a dark character study set in Mexico, that ranks among the finest he ever did. Prizzi's Honor on the other hand, a film loaded with star power, good intentions and a decent script, proves to be a major letdown.<br /><br />The overall tone and plot of a gangster falling in love with a female hit-man prefigures the quirky crimedies that caught Hollywood by storm in the early 90's but the script is too convoluted for its own sake, the motivations are off and on the whole the story seems unsure of what exactly it's trying to be: a romantic comedy, a crime drama, a gangster saga etc. Jack Nicholson (doing a Brooklyn accent that works perfectly for De Niro but sounds unconvincing coming from Jack) and Kathleen Turner in the leading roles seem to be in paycheck mode, just going through the motions almost sleepwalking their way through some parts. Anjelica Huston on the other hand fares better but her performance is sabotaged by her character's motivations: she starts out the victim of her bigot father's disdain, she proves to be supportive to her ex-husband, then becomes a vindictive bitch that wants his head on a plate.<br /><br />The colours of the movie have a washed-up quality like it was made in the early 70's and Huston's direction is as uninteresting as everything else. There's promise behind the story and perhaps in the hands of a director hungry to be recognized it could've been morphed to something better but what's left looks like a film nobody was really interested in making.\"\n",
            "0\n",
            "b'The concept of the legal gray area in Love Crimes contributes to about 10% of the movie\\'s appeal; the other 90% can be attributed to it\\'s flagrant bad-ness. To say that Sean Young\\'s performance as a so-called district attorney is wooden is a gross understatement. With her bland suits and superfluous hair gel, Young does a decent job at convincing the audience of her devout hatred for men. Why else would she ask her only friend to pose as a prostitute just so she can arrest cops who try to pick up on them? This hatred is also the only reason why she relentlessly pursues a perverted photographer who gives women a consensual thrill and the driving force behind this crappy movie. Watching Young go from frigid to full-frontal nudity does little to raise interest, but the temper tantrum she throws standing next to a fire by a lake does. Watching her rant and rave about her self-loathing and sexual frustration makes Love Crimes worth the rental fee, but it\\'s all downhill to and from there. Despite her urge to bring Patrick Bergin\\'s character to justice, her policing skills completely escape her in the throes of her own tired lust and passion. Patrick Bergin does a decent enough job as a slimy sociopath; if it worked in Sleeping With the Enemy it sure as hell can work in this. But I can\\'t help but wonder if the noticeable lack of energy Young brings to the film conflicts with his sliminess. I\\'m guessing it does and the result is a \"thriller\" with thrills that are thoroughly bad and yet comedic.'\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yt90kfX_bSi"
      },
      "source": [
        "Important: Your dataset is not prepared yet, you just explained how to load it !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3PxW6I71oSJ"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "From above, you see there are some noise in the data, for instance the `<br />` tags. So we want to remove them !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsrTEWZl42dw"
      },
      "source": [
        "Having looked at our data above, we see that the raw text contains HTML break\n",
        "tags of the form `<br />`. \n",
        "\n",
        "These tags will not be removed by the default standardizer (which doesn't strip HTML). Because of this, we will need to create a custom standardization function.\n",
        "\n",
        "Write a function `custom_standardisation` which take text as input, removes the tags of the form  `<br />` and proceeds to tradional standardisation: lowering caracters, removing punctuation. \n",
        "\n",
        "We recommend using the internal tensorflow methods inside `tf.strings`: `tf.strings.lower`, `tf.strings.regex_replace`, `string/punctuation`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsPY1TX2590M"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  # YOUR CODE HERE\n",
        "  output_data = tf.strings.lower(input_data)\n",
        "  output_data = tf.strings.regex_replace(output_data,'<br />',' ',replace_global=True)\n",
        "  output_data = tf.strings.regex_replace(output_data, \"[%s]\" % re.escape(string.punctuation), \"\")\n",
        "  return output_data"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbz59ACr5ul-"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu4rOFoN6L1E"
      },
      "source": [
        "Now we will use this function in a vectoriation workflow using `TextVectorization`. This helper will normalize, split, and map strings to integers. \n",
        "\n",
        "We suggest default values for the model constants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCLnGpak4kHS"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "sequence_length = 80\n",
        "\n",
        "# YOUR CODE HERE\n",
        "vectorize_layer = TextVectorization(max_tokens=max_features, standardize=custom_standardization, output_mode='int', output_sequence_length=sequence_length)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj3lioWq6-P1"
      },
      "source": [
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "    \n",
        "```markdown\n",
        "Have  loot at the documentation [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization)\n",
        "```\n",
        "    \n",
        "</details>\n",
        "​\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRgOFrhM7Lig"
      },
      "source": [
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaOBTHFF3j9i",
        "outputId": "9d5fe430-cd0a-4155-cb30-f4bc08bf44c0"
      },
      "source": [
        "#chck text_ds\n",
        "for text_batch in text_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Silent Night, Deadly Night 5 is the very last of the series, and like part 4, it\\'s unrelated to the first three except by title and the fact that it\\'s a Christmas-themed horror flick.<br /><br />Except to the oblivious, there\\'s some obvious things going on here...Mickey Rooney plays a toymaker named Joe Petto and his creepy son\\'s name is Pino. Ring a bell, anyone? Now, a little boy named Derek heard a knock at the door one evening, and opened it to find a present on the doorstep for him. Even though it said \"don\\'t open till Christmas\", he begins to open it anyway but is stopped by his dad, who scolds him and sends him to bed, and opens the gift himself. Inside is a little red ball that sprouts Santa arms and a head, and proceeds to kill dad. Oops, maybe he should have left well-enough alone. Of course Derek is then traumatized by the incident since he watched it from the stairs, but he doesn\\'t grow up to be some killer Santa, he just stops talking.<br /><br />There\\'s a mysterious stranger lurking around, who seems very interested in the toys that Joe Petto makes. We even see him buying a bunch when Derek\\'s mom takes him to the store to find a gift for him to bring him out of his trauma. And what exactly is this guy doing? Well, we\\'re not sure but he does seem to be taking these toys apart to see what makes them tick. He does keep his landlord from evicting him by promising him to pay him in cash the next day and presents him with a \"Larry the Larvae\" toy for his kid, but of course \"Larry\" is not a good toy and gets out of the box in the car and of course, well, things aren\\'t pretty.<br /><br />Anyway, eventually what\\'s going on with Joe Petto and Pino is of course revealed, and as with the old story, Pino is not a \"real boy\". Pino is probably even more agitated and naughty because he suffers from \"Kenitalia\" (a smooth plastic crotch) so that could account for his evil ways. And the identity of the lurking stranger is revealed too, and there\\'s even kind of a happy ending of sorts. Whee.<br /><br />A step up from part 4, but not much of one. Again, Brian Yuzna is involved, and Screaming Mad George, so some decent special effects, but not enough to make this great. A few leftovers from part 4 are hanging around too, like Clint Howard and Neith Hunter, but that doesn\\'t really make any difference. Anyway, I now have seeing the whole series out of my system. Now if I could get some of it out of my brain. 4 out of 5.'\n",
            "b\"This Italian film from the '70's is NOT even in the class with Dog Soldiers, The Howling, or even that awful American Werewolf in Paris, BUT...it is fun to watch. I'm talking about watching the lead actress, a stunning blonde, run amok in her birthday suit. We're talking about graphic, complete nudity...it's obvious that she is a real blonde...humma humma humma!! The story is a hoot, the SFX are childish, and the acting (for the most part) stinks. The only redeeming value of this movie is all (and there is a LOT) the nudity & sex scenes. Tame by HBO standards, but still fun to see when you find yourself without a date on Saturday night. OK...HERE'S THE SPOILER...There is NO werewolf (except in the opening scene of the heroine(??)'s ancestor. The girl just imagines that she's a werewolf...in other words, a clinical Lycanthrope.\"\n",
            "b'Mr Perlman gives a standout performance (as usual). Sadly, he has to struggle with an underwritten script and some nonsensical set pieces.<br /><br />Larsen is in \"Die Hard\" mode complete with singlet and bulging muscles, I\\'m sure he could do better but seems satisfied to grimace and snarl through his part.<br /><br />The lovely Erika is very decorative (even though fully clothed!) and shows some signs of \"getting\" acting at last.<br /><br />SFX are mainly poor CGI and steals from other movies.<br /><br />The shootouts are pitiful - worthy of the A-Team<br /><br />Not even worth seeing for Perlman - AVOID'\n",
            "b\"I'm a Christian who generally believes in the theology taught in Left Behind. That being said, I think Left Behind is one of the worst films I've seen in some time.<br /><br />To have a good movie, you need to have a well-written screenplay. Left Behind fell woefully short on this. For one thing, it radically deviates from the book. Sometimes this is done to condense a 400-page novel down to a two-hour film, but in this film I saw changes that made no sense whatsoever.<br /><br />Another thing, there is zero character development. When characters in the story get saved (I won't say who), the book makes it clear that it's a long, soul-searching process. In the film it's quick and artificial. The book is written decently enough where people like Rayford Steele, Buck Williams and Hattie Durham seem real, but in the movie scenarios are consistently given the quick treatment without anything substantial. In another scene where one character gets angry about being left behind (again, I won't say who), it seems artificial.<br /><br />I realize as a Christian it's unedifying for me to say I disliked this film, but I can't in a good conscience recommend a film that I feel was horribly done. Perhaps it would've been better to make the first book into 2-3 films. Either way, Christians need to realize that to be taken seriously as filmmakers, we need to start by putting together a film in a quality way. I realize a lot of effort probably went into Left Behind, but that's the way I see it.\"\n",
            "b'The Forest isn\\'t just your everyday standard slasher/backwoods cannibal fare, it also has an interesting mix of supernatural elements as well. The story is about two couples that hike into the forest on a camping trip. A cave dwelling, cannibalistic woodsmen and the ghosts of his dead wife and two children soon terrorize them. There is something you don\\'t see every slasher. Director Don Jones gets an \"A\" for effort although the film itself falls flat on just about every level, the acting is just simply average except for Jeanette Kelly who plays the dead wife of the woodsman (Michael Brody aka Gary Kent).<br /><br />The film opens with some beautiful shots of a couple hiking through a valley and into a forest. They realize too late that someone is stalking them. They are both dispatched in typical slasher fare. Our killer uses a trusty hunting knife throughout the entire film, except during a flashback when he implements a handsaw, pitchfork and rusty saw blade to dispatch his cheating wife\\'s lover.<br /><br />The Forest has a good story line but the movie just doesn\\'t work along with it I found it pretty boring with simply crappy acting. 4/10'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Sh0sif1oSK"
      },
      "source": [
        "### Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:\n",
        "\n",
        "```python\n",
        "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0PMhGOE1oSK"
      },
      "source": [
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two methods is that  option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training your model on the graphical processing unit, a.k.a GPU (this is the hardware on which a neural network is trained). This is the case since you run first your pre-processing on all the data and then train your model, so the first step can occur on the CPU, where it will be more efficient.\n",
        "\n",
        "\n",
        "Here since we're training the model on GPU, we want to go with this 2nd option to get the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input so we would use the first option. But we can actually recover this first option from a model trained with the second one. We do this in the last section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1qtC_bCN5f"
      },
      "source": [
        "Write a function you will apply to your 3 datasets (`raw_train_ds`, `raw_val_ds` and `raw_test_ds`) to vectorize them. Recall that each dataset contains tuple of `(text, label)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuTQBZqUCvU0"
      },
      "source": [
        "# This function should return 2 elements: the text vectorized and the label\n",
        "def vectorize_text(text, label):\n",
        "    # YOUR CODE HERE\n",
        "    vectorized = vectorize_layer(tf.expand_dims(text, -1))\n",
        "    return vectorized, label\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kImaPJuXCzef"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2wmmYEjCIAo"
      },
      "source": [
        "#aquestion : a quoi ca correspond ?\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGFyHuA_Lqtc",
        "outputId": "5b6e4d69-4c43-40f6-cc2f-869ddbedc93c"
      },
      "source": [
        "#why label 0 ?\n",
        "for text_batch, label_batch in train_ds.take(3):\n",
        "      print(text_batch.numpy()[0])\n",
        "      print(label_batch.numpy()[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   86    17   260     2   222     1   571    31   229    11  2418 10253\n",
            "    51    22    25   404   251    12   306   282     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "0\n",
            "[  10   13  747   51   10  208   11   19  450  548    9   13    2  113\n",
            "  121  814    5    2  644   87  665   10   13   87   97  274 1007 6768\n",
            "  325   68 1059   11 1177    7    4  743    6   54  511    2 9245   25\n",
            "  820    6 1222    8 1768    5  645 1246   60   23 2099   31    2  113\n",
            "    5  206   18   77 2419    2  232    5    2  840   14   15    2  173\n",
            " 7644 5372  158    4  239 2056 1881   18   92   97]\n",
            "0\n",
            "[  31   83   10  197    2 1675   59   26    4   50   70 1210   17   16\n",
            "   29 1826  111   87  747   13   10    5  259    2  111    7  585   28\n",
            "  247    7    8  115   16  104  348   18 2964  907    9    6  167   20\n",
            "    4 3401    1   16 2040    3 1044   16 5003    3 8537    2   95   27\n",
            " 4796    2 1093 8917    5    2 1234    2 2104    5    2 3156    5    2\n",
            "  247    6  192  348    2 1548  768   12  203  281]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK0anF0uvHG4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEvH3JZ1oSL"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "First, we choose a very simple architecture for our baseline model:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- 1 `LSTM` layer;\n",
        "- 1 `Dense` layer for the final prediction.\n",
        "\n",
        "We will use the `adam` optimizer. The task is to classify our entries between two classes, what should be the size of the final `Dense` layer and which loss will you use?\n",
        "\n",
        "Then you can try out more advanced architectures at the end!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiQIFqeXEJ8F"
      },
      "source": [
        "#see arguments of LSTM\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "\n",
        "\n",
        "def build_baseline_model(embedding_dim = 128):\n",
        "  model = Sequential()\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  # add your layers\n",
        "  model.add(Embedding(max_features,embedding_dim))\n",
        "  model.add(LSTM(128, dropout=0.2))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  # Compile the model \n",
        "  model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65VRrmmazW43"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def build_baseline_model(embedding_dim=32, conv_dim=128, num_filters=7):\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(LSTM(128, dropout=0.2))\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F59JWbAO1oSL"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rceSKY131oSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70a5e56-5a7d-475e-cd3a-9def3a82d39f"
      },
      "source": [
        "\n",
        "print(train_ds,val_ds)\n",
        "for example in val_ds.take(2):\n",
        "  print(example[0].numpy())\n",
        "  print(example[1].numpy())\n",
        "  exit()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((None, 80), (None,)), types: (tf.int64, tf.int32)> <PrefetchDataset shapes: ((None, 80), (None,)), types: (tf.int64, tf.int32)>\n",
            "[[   4  469  495 ...  468    5  918]\n",
            " [   2  136   23 ...    0    0    0]\n",
            " [ 254   10  237 ...  262    4  171]\n",
            " ...\n",
            " [   4   52 4868 ...  322 1176   51]\n",
            " [   4  590    5 ...  289    9  718]\n",
            " [  10  606    6 ...    3   24  116]]\n",
            "[0 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 1 1]\n",
            "[[2820 1598    7 ...  178    4  356]\n",
            " [  11   17   43 ... 2555 8096  120]\n",
            " [  10  495   11 ...  443   20  971]\n",
            " ...\n",
            " [  11 1264  757 ... 4724    1    3]\n",
            " [  11   19   43 ...    3 1223   50]\n",
            " [ 218   14 2225 ...  182    5    2]]\n",
            "[0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP1BhwUJAzKU",
        "outputId": "b1d80dd6-4845-42ae-9b9a-e16304f2c26e"
      },
      "source": [
        "#ca plante !!\n",
        "epochs = 9\n",
        "\n",
        "model = build_baseline_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "625/625 [==============================] - 57s 40ms/step - loss: 0.5749 - accuracy: 0.6800 - val_loss: 0.4512 - val_accuracy: 0.7986\n",
            "Epoch 2/9\n",
            "625/625 [==============================] - 20s 32ms/step - loss: 0.3283 - accuracy: 0.8673 - val_loss: 0.4861 - val_accuracy: 0.8116\n",
            "Epoch 3/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.2117 - accuracy: 0.9212 - val_loss: 0.5478 - val_accuracy: 0.7988\n",
            "Epoch 4/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.1405 - accuracy: 0.9508 - val_loss: 0.6940 - val_accuracy: 0.7786\n",
            "Epoch 5/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.1199 - accuracy: 0.9581 - val_loss: 0.7801 - val_accuracy: 0.7990\n",
            "Epoch 6/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.0696 - accuracy: 0.9772 - val_loss: 0.9482 - val_accuracy: 0.7972\n",
            "Epoch 7/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.0630 - accuracy: 0.9793 - val_loss: 0.9347 - val_accuracy: 0.7994\n",
            "Epoch 8/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.0540 - accuracy: 0.9837 - val_loss: 0.9848 - val_accuracy: 0.7958\n",
            "Epoch 9/9\n",
            "625/625 [==============================] - 19s 31ms/step - loss: 0.0330 - accuracy: 0.9909 - val_loss: 1.1568 - val_accuracy: 0.7982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HjuY0b91oSM"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEeKDGk3vnSO"
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "FUmksMuIqhNg",
        "outputId": "3272aea2-7598-40aa-96aa-afea0bd576de"
      },
      "source": [
        "# Plot history\n",
        "plot_history(history)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bf4b53816a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld6eBpRTv6vr"
      },
      "source": [
        "Apply your model on your test dataset and evaluate its predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_enDofC1oSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1274863-148c-44ca-9a3e-05d380829000"
      },
      "source": [
        "evaluations = model.evaluate(test_ds)\n",
        "test_evaluations = {name: value for value, name in zip(evaluations, model.metrics_names)}\n",
        "print(test_evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 1s 5ms/step - loss: 0.9089 - accuracy: 0.7532\n",
            "{'loss': 0.9089375138282776, 'accuracy': 0.7531999945640564}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4WFibkSv_zH"
      },
      "source": [
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "    \n",
        "```markdown\n",
        "Check the `model.evaluate` function documentation (how could you retrieve the name of the float returned?)\n",
        "```\n",
        "    \n",
        "</details>\n",
        "​\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "evaluations = model.evaluate(test_ds)\n",
        "test_evaluations = {name: value for value, name in zip(evaluations, model.metrics_names)}\n",
        "print(test_evaluations)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrdU5KvF1oSO"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model, using the baseline you just defined and adding as a first step the vectorize layer. You can again do that with the Keras `Sequential` API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GLnYJsW1oSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968d144a-a671-4369-de21-b9a3d0bfdd45"
      },
      "source": [
        "end2end_model = tf.keras.Sequential()\n",
        "end2end_model.add(tf.keras.Input(shape=(1,), dtype=\"string\"))\n",
        "end2end_model.add(vectorize_layer)\n",
        "end2end_model.add(model)\n",
        "\n",
        "\n",
        "end_to_end_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 9s 41ms/step - loss: 0.9130 - accuracy: 0.7528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9089372754096985, 0.7531999945640564]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwYyBRVyw9ld"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "end2end_model = tf.keras.models.Sequential()\n",
        "# A string input\n",
        "end2end_model.add(tf.keras.Input(shape=(1,), dtype=\"string\"))\n",
        "# Turn strings into vocab indices\n",
        "end2end_model.add(vectorize_layer)\n",
        "end2end_model.add(model)\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox8sDbBCxJ-P"
      },
      "source": [
        "You can now apply this model on any input, so give it a try !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVI0UB2frJTE"
      },
      "source": [
        "# Create your own input\n",
        "my_input_good = \"I liked very much this movie\"\n",
        "my_input_good_2 = \"This movie is not very bad though.\"\n",
        "my_input_good_3 = \"I fall asleep\"\n",
        "my_input_bad = \"This movie is terribly bad\"\n",
        "\n",
        "# You just need to convert your string into a tensor\n",
        "# YOUR CODE HERE\n",
        "text_input = tf.convert_to_tensor([my_input_good, my_input_good_2, my_input_good_3, my_input_bad], dtype=tf.string)\n",
        "end_to_end_model.predict(text_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBZRJCcxSjp"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "text_input = tf.convert_to_tensor([my_input_good, my_input_good_2, my_input_good_3, my_input_bad], dtype=tf.string)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Hbt43LFLPz"
      },
      "source": [
        "## Try out more complex architectures !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWX9w89du4VD"
      },
      "source": [
        "### Bi-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4bAEdDyMJs"
      },
      "source": [
        "Here we suggest the following architecture:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- 1 or several `Bidirectional` layers (if multiple you need the first one to be a many-to-many one !)\n",
        "- 1 Dense layer to add more complexity;\n",
        "- 1 Dense layer for the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpO7DkMyLee"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "def build_bi_lstm_model(embedding_dim=128, dense_dim=128):\n",
        "  # YOUR CODE HERE\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1yc6fgPytSS"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def build_bi_lstm_model(embedding_dim=128, dense_dim=128):\n",
        "  # A integer input for vocab indices.\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Conv1D + global max pooling\n",
        "  model.add(Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "  model.add(Bidirectional(layers.LSTM(64)))\n",
        "\n",
        "  # We add a vanilla hidden layer:\n",
        "  model.add(Dense(dense_dim, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "  model.add(Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99W-_t4_ymEF",
        "outputId": "99c139f4-115d-4139-ad20-372e5f62cc3b"
      },
      "source": [
        "epochs = 9\n",
        "\n",
        "bi_lstm_model = build_bi_lstm_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "bi_lstm_history = bi_lstm_model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 14s 57ms/step - loss: 0.6162 - accuracy: 0.6141 - val_loss: 0.3937 - val_accuracy: 0.8246\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.3526 - accuracy: 0.8475 - val_loss: 0.4185 - val_accuracy: 0.8196\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.2469 - accuracy: 0.9029 - val_loss: 0.4731 - val_accuracy: 0.8054\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.1725 - accuracy: 0.9358 - val_loss: 0.5011 - val_accuracy: 0.8052\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.1241 - accuracy: 0.9548 - val_loss: 0.7450 - val_accuracy: 0.7734\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0977 - accuracy: 0.9640 - val_loss: 0.7192 - val_accuracy: 0.8006\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0710 - accuracy: 0.9740 - val_loss: 0.8597 - val_accuracy: 0.8040\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0656 - accuracy: 0.9768 - val_loss: 1.0517 - val_accuracy: 0.7664\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0560 - accuracy: 0.9810 - val_loss: 1.1177 - val_accuracy: 0.7858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLQAdsm9u73Y"
      },
      "source": [
        "### 1D-Convnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csnUisqWu68g"
      },
      "source": [
        "Here we suggest the following architecture:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- eventually a Dropout after for regularization;\n",
        "- 2 layers 1D `Conv1D`\n",
        "- 1 GlobalMaxPooling1D layer to pool;\n",
        "- 1 Dense layer for the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mqN3y3h1oSL"
      },
      "source": [
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "def build_cnn_model(embedding_dim=128, conv_dim=128, num_filters=7):\n",
        "  # YOUR CODE HERE\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdHWfy9y1wN"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "def build_cnn_model(embedding_dim=128, conv_dim=128, num_filters=7):\n",
        "  # A integer input for vocab indices.\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Conv1D + global max pooling\n",
        "  model.add(Conv1D(conv_dim, num_filters, padding=\"valid\", activation=\"relu\", strides=3))\n",
        "  model.add(Conv1D(conv_dim, num_filters, padding=\"valid\", activation=\"relu\", strides=3))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "\n",
        "  # We add a vanilla hidden layer:\n",
        "  model.add(Dense(conv_dim, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "  model.add(Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iklP87wtFOEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e099731-8278-49c5-d404-e2bfd4da8327"
      },
      "source": [
        "epochs = 9\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "cnn_history = cnn_model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 6s 32ms/step - loss: 0.6861 - accuracy: 0.5211 - val_loss: 0.4645 - val_accuracy: 0.7806\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.4473 - accuracy: 0.7964 - val_loss: 0.4323 - val_accuracy: 0.8102\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.2618 - accuracy: 0.8996 - val_loss: 0.5236 - val_accuracy: 0.7990\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.1451 - accuracy: 0.9495 - val_loss: 0.6559 - val_accuracy: 0.7948\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0952 - accuracy: 0.9660 - val_loss: 0.8134 - val_accuracy: 0.7830\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0773 - accuracy: 0.9716 - val_loss: 0.8962 - val_accuracy: 0.7988\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0497 - accuracy: 0.9830 - val_loss: 1.0438 - val_accuracy: 0.7940\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0206 - accuracy: 0.9924 - val_loss: 1.1188 - val_accuracy: 0.7956\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 1.1047 - val_accuracy: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
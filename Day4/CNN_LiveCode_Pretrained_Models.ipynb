{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CNN-LiveCode-Pretrained-Models",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L4ncelot1024/Learn_Deep_Learning_Le_Wagon/blob/main/Day4/CNN_LiveCode_Pretrained_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHK6DyunSbs4"
      },
      "source": [
        "# Live Code: Pretrained Models\n",
        "\n",
        "Let's see in more details the classical CNN architectures which are popular and how to apply them !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auRhmaDC6BOG"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvdriJS5-l5"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVlROgOu6ag-"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "Here we download and process the data like in the exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9O0z72L6hV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6df6eb1-5c91-4055-b417-d09204d9b441"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-26 06:10:33--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.128, 142.250.107.128, 173.194.202.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  82.6MB/s    in 0.8s    \n",
            "\n",
            "2021-04-26 06:10:34 (82.6 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG2j89gb6hYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8067f11f-94b8-4683-a8d6-1f8706a1f937"
      },
      "source": [
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our test cat pictures\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "\n",
        "# Directory with our test dog pictures\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "\n",
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "train_dog_fnames.sort()\n",
        "\n",
        "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
        "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
        "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images: 1000\n",
            "total training dog images: 1000\n",
            "total test cat images: 500\n",
            "total test dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S73I1G_2Hrm7"
      },
      "source": [
        "## Data Augmentation\n",
        "\n",
        "Since the dataset is very small, we add a data augmentation step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2LcDhoe6hap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7143a284-a8c0-4eb3-81d2-e2f7ae2d60e2"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255, for the train generator we set\n",
        "# the validation split.\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, \n",
        "                                   width_shift_range = 0.2, height_shift_range = 0.2, \n",
        "                                   shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# global settings\n",
        "target_size = (224, 224)\n",
        "batch_size = 20\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "print('Train Dataset')\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=target_size,  # All images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "print('Test Dataset')\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Dataset\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Test Dataset\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI5rmt4UBwXs"
      },
      "source": [
        "## Pre-trained Model\n",
        "\n",
        "One thing that is commonly done in computer vision is to take a model trained on a very large dataset, run it on your own, smaller dataset, and extract the intermediate representations (features) that the model generates. These representations are frequently informative for your own computer vision task, even though the task may be quite different from the problem that the original model was trained on. This versatility and repurposability of convnets is one of the most interesting aspects of deep learning.\n",
        "\n",
        "We will apply this method using 4 popular architecture for which Keras provides pre-trained models (but there are way more to test [here](https://keras.io/api/applications/)\n",
        "\n",
        "For each model, the strategy is the same: \n",
        "1. First, you need to adapt your image to those used for training the loaded model if it accepts only this side. Here this requires to re-run the initialisation of the data generator from above;\n",
        "2. Then, you need to identify which intermediate layer of the model to use for feature extraction. A common practice is to use the output of the antepenultieme layer (before the classifier layer). You will remove layers defined in the network after this one.\n",
        "3. Freeze the layers you don't want to finetune (here we won't do any fine-tuning, but in the exercise you see how to do it);\n",
        "4. You then need to add a final layer to solve your task and output your expected target, here it will be a simple binary classifier layer.\n",
        "5. Finally, you can train your model !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyO8KR-nIKve"
      },
      "source": [
        "### VGG-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZWUXuNKINBp",
        "outputId": "8436efc9-9c35-43da-dce6-ae20c8550db1"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "# Step 1 and 2\n",
        "base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images\n",
        "include_top = False, # Leave out the last fully connected layer\n",
        "weights = 'imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eqmYh1jINET"
      },
      "source": [
        "# Step 3\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C0ZEOxMINGd"
      },
      "source": [
        "# Step 4\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(base_model.output)\n",
        "\n",
        "# Add a fully connected layer with 512 hidden units and ReLU activation\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "\n",
        "# Add a dropout rate of 0.5 \n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(base_model.input, x)\n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEJJmncTKh_V",
        "outputId": "aa9ff5f7-4af0-4f6b-9de7-f1ba4dbe8c51"
      },
      "source": [
        "# Step 5\n",
        "vgghist = model.fit(train_generator, validation_data=test_generator, \n",
        "                    steps_per_epoch=100, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 69s 323ms/step - loss: 0.9805 - acc: 0.6100 - val_loss: 0.3319 - val_acc: 0.8680\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.4791 - acc: 0.7647 - val_loss: 0.2592 - val_acc: 0.9020\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.4490 - acc: 0.8062 - val_loss: 0.3353 - val_acc: 0.8480\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 32s 322ms/step - loss: 0.3934 - acc: 0.8213 - val_loss: 0.2176 - val_acc: 0.9170\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 32s 323ms/step - loss: 0.3921 - acc: 0.8317 - val_loss: 0.2678 - val_acc: 0.8880\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3696 - acc: 0.8372 - val_loss: 0.2473 - val_acc: 0.8970\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3379 - acc: 0.8558 - val_loss: 0.2150 - val_acc: 0.9140\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3388 - acc: 0.8543 - val_loss: 0.2095 - val_acc: 0.9100\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 32s 320ms/step - loss: 0.3447 - acc: 0.8590 - val_loss: 0.2002 - val_acc: 0.9250\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 32s 321ms/step - loss: 0.3116 - acc: 0.8645 - val_loss: 0.1867 - val_acc: 0.9310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK95OfId__yG"
      },
      "source": [
        "### Inception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BadkoyfLNtl",
        "outputId": "1e8de134-c1e9-476d-9b1e-7743274001e4"
      },
      "source": [
        "# Step 1\n",
        "target_size = (150, 150)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, \n",
        "                                                    class_mode = 'binary', target_size = target_size)\n",
        "test_generator = test_datagen.flow_from_directory(test_dir, batch_size = 20, \n",
        "                                                        class_mode = 'binary', target_size = target_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qcoXQq_LOK8",
        "outputId": "e0455ad5-42d4-451a-83e8-3c6631cc8238"
      },
      "source": [
        "# Step 2\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "base_model = InceptionV3(input_shape = list(target_size) + [3] , \n",
        "                         include_top = False, weights = 'imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGxfioieLOQj"
      },
      "source": [
        "# Step 3\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBKySAqwLOUt"
      },
      "source": [
        "# Step 4\n",
        "x = layers.Flatten()(base_model.output)\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(base_model.input, x)\n",
        "\n",
        "model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auY3E7F90ZAA",
        "outputId": "c0069b7c-0d5a-4433-d3f7-3186f8914c56"
      },
      "source": [
        "inc_history = model.fit_generator(train_generator, validation_data=test_generator,\n",
        "                                  steps_per_epoch = 100, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 56s 198ms/step - loss: 4.1352 - acc: 0.7667 - val_loss: 0.1446 - val_acc: 0.9530\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.3763 - acc: 0.8979 - val_loss: 0.4961 - val_acc: 0.8950\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.3648 - acc: 0.8996 - val_loss: 0.1306 - val_acc: 0.9560\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 19s 188ms/step - loss: 0.4215 - acc: 0.8998 - val_loss: 0.1560 - val_acc: 0.9530\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 19s 186ms/step - loss: 0.3238 - acc: 0.9069 - val_loss: 0.3476 - val_acc: 0.9100\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 19s 185ms/step - loss: 0.3206 - acc: 0.9093 - val_loss: 0.2014 - val_acc: 0.9490\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.2809 - acc: 0.9268 - val_loss: 0.1436 - val_acc: 0.9550\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 18s 185ms/step - loss: 0.2612 - acc: 0.9225 - val_loss: 0.1918 - val_acc: 0.9510\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 18s 184ms/step - loss: 0.2706 - acc: 0.9249 - val_loss: 0.2002 - val_acc: 0.9490\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 19s 187ms/step - loss: 0.2733 - acc: 0.9339 - val_loss: 0.1463 - val_acc: 0.9650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuCi7Vm_LOnn"
      },
      "source": [
        "### ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEoNvlugLTic",
        "outputId": "d5a57d6b-fe40-4382-fcf1-296255d3a16e"
      },
      "source": [
        "# Step 1\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=target_size,  # All images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5MppUmDLT4l",
        "outputId": "82602b24-2d2d-4a37-f636-9f84b0e94c73"
      },
      "source": [
        "# Step 2\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "base_model = ResNet50(input_shape=list(target_size) + [3] , include_top=False, weights=\"imagenet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUv8EHm8LT7K"
      },
      "source": [
        "# Step 3\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ELgz-fHLT9n"
      },
      "source": [
        "# Step 4 - NB: here we use the tensorflow Sequential framework to simplify the model creation\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "base_model = Sequential()\n",
        "base_model.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
        "base_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "base_model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsQGHXyW1TRU",
        "outputId": "6b561ed0-d7c0-452a-e27e-9359a9cab4b7"
      },
      "source": [
        "# Step 5\n",
        "resnet_history = base_model.fit(train_generator, validation_data = test_generator, \n",
        "                                steps_per_epoch = 100, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 49s 380ms/step - loss: 0.7490 - acc: 0.8498 - val_loss: 14.0817 - val_acc: 0.5000\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.2975 - acc: 0.9391 - val_loss: 5.3193 - val_acc: 0.5000\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.2224 - acc: 0.9540 - val_loss: 9.1252 - val_acc: 0.5000\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 37s 368ms/step - loss: 0.1860 - acc: 0.9587 - val_loss: 1.7131 - val_acc: 0.4880\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 37s 371ms/step - loss: 0.1596 - acc: 0.9638 - val_loss: 4.4278 - val_acc: 0.5020\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 37s 370ms/step - loss: 0.1419 - acc: 0.9704 - val_loss: 4.9223 - val_acc: 0.5010\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 37s 369ms/step - loss: 0.1179 - acc: 0.9766 - val_loss: 0.8491 - val_acc: 0.7830\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 37s 368ms/step - loss: 0.1279 - acc: 0.9711 - val_loss: 1.4939 - val_acc: 0.7670\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 37s 366ms/step - loss: 0.0978 - acc: 0.9764 - val_loss: 0.2048 - val_acc: 0.9570\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 37s 367ms/step - loss: 0.0732 - acc: 0.9774 - val_loss: 1.3418 - val_acc: 0.9050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGuRFaXLUNW"
      },
      "source": [
        "### EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8mVkq7D1vpl",
        "outputId": "4d2895e2-09c2-4965-e715-463b361a57c2"
      },
      "source": [
        "# Step 0 - For this model we need to add an extra dependency\n",
        "!pip install -U efficientnet\n",
        "\n",
        "import efficientnet.keras as efn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet\n",
            "  Downloading https://files.pythonhosted.org/packages/53/97/84f88e581d6ac86dcf1ab347c497c4c568c38784e3a2bd659b96912ab793/efficientnet-1.1.1-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet) (0.16.2)\n",
            "Collecting keras-applications<=1.0.8,>=1.0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.5.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.15.0)\n",
            "Installing collected packages: keras-applications, efficientnet\n",
            "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwPYb5tyLXkl",
        "outputId": "b861d2ea-e60f-4fdd-ac28-6bd5c763bc08"
      },
      "source": [
        "# Step 1\n",
        "target_size = (224, 224)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=target_size,  # All images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using val_datagen generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3-NjYgnLYEo",
        "outputId": "11f23ad0-9f1a-4095-f054-41a5e41ebc06"
      },
      "source": [
        "# Step 2\n",
        "base_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
            "16809984/16804768 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQcO7c8bLYHe"
      },
      "source": [
        "# Step 3\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBlHKf4uLYhi"
      },
      "source": [
        "# Step 4\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "predictions = Dense(1, activation=\"sigmoid\")(x)\n",
        "model_final = Model(inputs = base_model.input, outputs = predictions)\n",
        "\n",
        "model_final.compile(RMSprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy',metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0M57Zpo2QAI",
        "outputId": "d2af37fb-d4ce-423c-fc78-00eeaee659a1"
      },
      "source": [
        "# Step 5\n",
        "eff_history = model_final.fit_generator(train_generator, validation_data = test_generator,\n",
        "                                        steps_per_epoch = 100, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "100/100 [==============================] - 38s 318ms/step - loss: 0.5137 - accuracy: 0.8591 - val_loss: 0.1120 - val_accuracy: 0.9730\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 31s 305ms/step - loss: 0.2503 - accuracy: 0.9269 - val_loss: 0.1325 - val_accuracy: 0.9740\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 30s 304ms/step - loss: 0.2221 - accuracy: 0.9497 - val_loss: 0.1579 - val_accuracy: 0.9720\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 31s 305ms/step - loss: 0.1635 - accuracy: 0.9499 - val_loss: 0.1649 - val_accuracy: 0.9790\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.2008 - accuracy: 0.9627 - val_loss: 0.2073 - val_accuracy: 0.9740\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 31s 306ms/step - loss: 0.2361 - accuracy: 0.9413 - val_loss: 0.2639 - val_accuracy: 0.9720\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 31s 305ms/step - loss: 0.2284 - accuracy: 0.9596 - val_loss: 0.3022 - val_accuracy: 0.9740\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 31s 311ms/step - loss: 0.3067 - accuracy: 0.9455 - val_loss: 0.2544 - val_accuracy: 0.9740\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 31s 308ms/step - loss: 0.2887 - accuracy: 0.9540 - val_loss: 0.3687 - val_accuracy: 0.9790\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 31s 309ms/step - loss: 0.3502 - accuracy: 0.9509 - val_loss: 0.4317 - val_accuracy: 0.9760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K55ssROu4sSl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
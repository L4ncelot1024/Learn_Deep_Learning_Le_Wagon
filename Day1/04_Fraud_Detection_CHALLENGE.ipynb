{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-Fraud-Detection_CHALLENGE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L4ncelot1024/Learn_Deep_Learning_Le_Wagon/blob/main/Day1/04_Fraud_Detection_CHALLENGE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXc07XfbxCAd"
      },
      "source": [
        "# Fraud Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-G-zdvsLsYw"
      },
      "source": [
        "In this exercise, we work on tabular data of credit card history available on [Kaggle](https://www.kaggle.com/mlg-ulb/creditcardfraud). The aim is to detect a mere 492 fraudulent transactions from 284,807 transactions in total.\n",
        "\n",
        "One particularity of this dataset is it's proportion of positive labels which is quite low, we say it's a highly imbalanced dataset since the number of examples in one class greatly outnumbers the examples. We'll see how to deal with such a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcZ0pUcPLH7V"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eidr7kJID2C6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "e2f34f14-e8ee-400a-88e4-74955ef67461"
      },
      "source": [
        "# This is to fix bugs seaborn has with new matplotlib version\n",
        "!pip install matplotlib==3.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting matplotlib==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/cb/a34046e75c9a4ecaf426ae0d0eada97078c8ce4bbe3250940b1a312a1385/matplotlib-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 21.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.0) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.0) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.0) (1.19.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.0) (1.15.0)\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: matplotlib\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed matplotlib-3.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AuhBJ4CLp88"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import tempfile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69W-FK8UyKfc"
      },
      "source": [
        "# This will fix the figsize for the whole notebook\n",
        "mpl.rcParams['figure.figsize'] = (15, 10)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f5NaFcRMLZq"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YKUBF6WMSu5"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiSre6FtMUf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "11660160-1fe7-488f-c996-14862ba8d8cf"
      },
      "source": [
        "file = tf.keras.utils\n",
        "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')\n",
        "raw_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlUbcqeNMYZb"
      },
      "source": [
        "### Inspecting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F5AFPJWMctv",
        "outputId": "a6d032fa-d010-435e-9abd-8bd17eed4581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# TODO: inspect the shape, format, basic statistics and proportion of positive sample of the data\n",
        "# (both for input and output)\n",
        "print(raw_df.shape)\n",
        "fraud_fraction = 100 * raw_df.Class.sum() / len(raw_df)\n",
        "print(f'{fraud_fraction} % of fraud in the dataset')\n",
        "raw_df.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(284807, 31)\n",
            "0.1727485630620034 % of fraud in the dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>284807.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94813.859575</td>\n",
              "      <td>3.919560e-15</td>\n",
              "      <td>5.688174e-16</td>\n",
              "      <td>-8.769071e-15</td>\n",
              "      <td>2.782312e-15</td>\n",
              "      <td>-1.552563e-15</td>\n",
              "      <td>2.010663e-15</td>\n",
              "      <td>-1.694249e-15</td>\n",
              "      <td>-1.927028e-16</td>\n",
              "      <td>-3.137024e-15</td>\n",
              "      <td>1.768627e-15</td>\n",
              "      <td>9.170318e-16</td>\n",
              "      <td>-1.810658e-15</td>\n",
              "      <td>1.693438e-15</td>\n",
              "      <td>1.479045e-15</td>\n",
              "      <td>3.482336e-15</td>\n",
              "      <td>1.392007e-15</td>\n",
              "      <td>-7.528491e-16</td>\n",
              "      <td>4.328772e-16</td>\n",
              "      <td>9.049732e-16</td>\n",
              "      <td>5.085503e-16</td>\n",
              "      <td>1.537294e-16</td>\n",
              "      <td>7.959909e-16</td>\n",
              "      <td>5.367590e-16</td>\n",
              "      <td>4.458112e-15</td>\n",
              "      <td>1.453003e-15</td>\n",
              "      <td>1.699104e-15</td>\n",
              "      <td>-3.660161e-16</td>\n",
              "      <td>-1.206049e-16</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>0.001727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "      <td>1.516255e+00</td>\n",
              "      <td>1.415869e+00</td>\n",
              "      <td>1.380247e+00</td>\n",
              "      <td>1.332271e+00</td>\n",
              "      <td>1.237094e+00</td>\n",
              "      <td>1.194353e+00</td>\n",
              "      <td>1.098632e+00</td>\n",
              "      <td>1.088850e+00</td>\n",
              "      <td>1.020713e+00</td>\n",
              "      <td>9.992014e-01</td>\n",
              "      <td>9.952742e-01</td>\n",
              "      <td>9.585956e-01</td>\n",
              "      <td>9.153160e-01</td>\n",
              "      <td>8.762529e-01</td>\n",
              "      <td>8.493371e-01</td>\n",
              "      <td>8.381762e-01</td>\n",
              "      <td>8.140405e-01</td>\n",
              "      <td>7.709250e-01</td>\n",
              "      <td>7.345240e-01</td>\n",
              "      <td>7.257016e-01</td>\n",
              "      <td>6.244603e-01</td>\n",
              "      <td>6.056471e-01</td>\n",
              "      <td>5.212781e-01</td>\n",
              "      <td>4.822270e-01</td>\n",
              "      <td>4.036325e-01</td>\n",
              "      <td>3.300833e-01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.041527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "      <td>-4.832559e+01</td>\n",
              "      <td>-5.683171e+00</td>\n",
              "      <td>-1.137433e+02</td>\n",
              "      <td>-2.616051e+01</td>\n",
              "      <td>-4.355724e+01</td>\n",
              "      <td>-7.321672e+01</td>\n",
              "      <td>-1.343407e+01</td>\n",
              "      <td>-2.458826e+01</td>\n",
              "      <td>-4.797473e+00</td>\n",
              "      <td>-1.868371e+01</td>\n",
              "      <td>-5.791881e+00</td>\n",
              "      <td>-1.921433e+01</td>\n",
              "      <td>-4.498945e+00</td>\n",
              "      <td>-1.412985e+01</td>\n",
              "      <td>-2.516280e+01</td>\n",
              "      <td>-9.498746e+00</td>\n",
              "      <td>-7.213527e+00</td>\n",
              "      <td>-5.449772e+01</td>\n",
              "      <td>-3.483038e+01</td>\n",
              "      <td>-1.093314e+01</td>\n",
              "      <td>-4.480774e+01</td>\n",
              "      <td>-2.836627e+00</td>\n",
              "      <td>-1.029540e+01</td>\n",
              "      <td>-2.604551e+00</td>\n",
              "      <td>-2.256568e+01</td>\n",
              "      <td>-1.543008e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "      <td>-8.903648e-01</td>\n",
              "      <td>-8.486401e-01</td>\n",
              "      <td>-6.915971e-01</td>\n",
              "      <td>-7.682956e-01</td>\n",
              "      <td>-5.540759e-01</td>\n",
              "      <td>-2.086297e-01</td>\n",
              "      <td>-6.430976e-01</td>\n",
              "      <td>-5.354257e-01</td>\n",
              "      <td>-7.624942e-01</td>\n",
              "      <td>-4.055715e-01</td>\n",
              "      <td>-6.485393e-01</td>\n",
              "      <td>-4.255740e-01</td>\n",
              "      <td>-5.828843e-01</td>\n",
              "      <td>-4.680368e-01</td>\n",
              "      <td>-4.837483e-01</td>\n",
              "      <td>-4.988498e-01</td>\n",
              "      <td>-4.562989e-01</td>\n",
              "      <td>-2.117214e-01</td>\n",
              "      <td>-2.283949e-01</td>\n",
              "      <td>-5.423504e-01</td>\n",
              "      <td>-1.618463e-01</td>\n",
              "      <td>-3.545861e-01</td>\n",
              "      <td>-3.171451e-01</td>\n",
              "      <td>-3.269839e-01</td>\n",
              "      <td>-7.083953e-02</td>\n",
              "      <td>-5.295979e-02</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "      <td>1.798463e-01</td>\n",
              "      <td>-1.984653e-02</td>\n",
              "      <td>-5.433583e-02</td>\n",
              "      <td>-2.741871e-01</td>\n",
              "      <td>4.010308e-02</td>\n",
              "      <td>2.235804e-02</td>\n",
              "      <td>-5.142873e-02</td>\n",
              "      <td>-9.291738e-02</td>\n",
              "      <td>-3.275735e-02</td>\n",
              "      <td>1.400326e-01</td>\n",
              "      <td>-1.356806e-02</td>\n",
              "      <td>5.060132e-02</td>\n",
              "      <td>4.807155e-02</td>\n",
              "      <td>6.641332e-02</td>\n",
              "      <td>-6.567575e-02</td>\n",
              "      <td>-3.636312e-03</td>\n",
              "      <td>3.734823e-03</td>\n",
              "      <td>-6.248109e-02</td>\n",
              "      <td>-2.945017e-02</td>\n",
              "      <td>6.781943e-03</td>\n",
              "      <td>-1.119293e-02</td>\n",
              "      <td>4.097606e-02</td>\n",
              "      <td>1.659350e-02</td>\n",
              "      <td>-5.213911e-02</td>\n",
              "      <td>1.342146e-03</td>\n",
              "      <td>1.124383e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "      <td>1.027196e+00</td>\n",
              "      <td>7.433413e-01</td>\n",
              "      <td>6.119264e-01</td>\n",
              "      <td>3.985649e-01</td>\n",
              "      <td>5.704361e-01</td>\n",
              "      <td>3.273459e-01</td>\n",
              "      <td>5.971390e-01</td>\n",
              "      <td>4.539234e-01</td>\n",
              "      <td>7.395934e-01</td>\n",
              "      <td>6.182380e-01</td>\n",
              "      <td>6.625050e-01</td>\n",
              "      <td>4.931498e-01</td>\n",
              "      <td>6.488208e-01</td>\n",
              "      <td>5.232963e-01</td>\n",
              "      <td>3.996750e-01</td>\n",
              "      <td>5.008067e-01</td>\n",
              "      <td>4.589494e-01</td>\n",
              "      <td>1.330408e-01</td>\n",
              "      <td>1.863772e-01</td>\n",
              "      <td>5.285536e-01</td>\n",
              "      <td>1.476421e-01</td>\n",
              "      <td>4.395266e-01</td>\n",
              "      <td>3.507156e-01</td>\n",
              "      <td>2.409522e-01</td>\n",
              "      <td>9.104512e-02</td>\n",
              "      <td>7.827995e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "      <td>9.382558e+00</td>\n",
              "      <td>1.687534e+01</td>\n",
              "      <td>3.480167e+01</td>\n",
              "      <td>7.330163e+01</td>\n",
              "      <td>1.205895e+02</td>\n",
              "      <td>2.000721e+01</td>\n",
              "      <td>1.559499e+01</td>\n",
              "      <td>2.374514e+01</td>\n",
              "      <td>1.201891e+01</td>\n",
              "      <td>7.848392e+00</td>\n",
              "      <td>7.126883e+00</td>\n",
              "      <td>1.052677e+01</td>\n",
              "      <td>8.877742e+00</td>\n",
              "      <td>1.731511e+01</td>\n",
              "      <td>9.253526e+00</td>\n",
              "      <td>5.041069e+00</td>\n",
              "      <td>5.591971e+00</td>\n",
              "      <td>3.942090e+01</td>\n",
              "      <td>2.720284e+01</td>\n",
              "      <td>1.050309e+01</td>\n",
              "      <td>2.252841e+01</td>\n",
              "      <td>4.584549e+00</td>\n",
              "      <td>7.519589e+00</td>\n",
              "      <td>3.517346e+00</td>\n",
              "      <td>3.161220e+01</td>\n",
              "      <td>3.384781e+01</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time            V1  ...         Amount          Class\n",
              "count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n",
              "mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n",
              "std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n",
              "min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n",
              "25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n",
              "50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n",
              "75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n",
              "max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkCnAtmxUPi"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "print(raw_df.shape)\n",
        "fraud_fraction = 100 * raw_df.Class.sum() / len(raw_df)\n",
        "print(f'{fraud_fraction} % of fraud in the dataset')\n",
        "raw.describe()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P-VfYfhNtyL"
      },
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg4-L6WDN0IS"
      },
      "source": [
        "#questio : why epsilon ?\n",
        "# TODO: Inspect and clean column by column:\n",
        "cleaned_df = raw_df.copy()\n",
        " \n",
        "# You don't want the `Time` column.\n",
        "cleaned_df.pop('Time')\n",
        " \n",
        "# The `Amount` column covers a huge range. Convert to log-space.\n",
        "eps=0.001 # 0 => 0.1¢\n",
        "cleaned_df['Amount'] = np.log(cleaned_df['Amount'] + eps)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rCpgzixg4Q"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "# hints:\n",
        "# - do you need the time column?\n",
        "# - what do you think about the range of the column Amount?\n",
        "\n",
        "cleaned_df = raw_df.copy()\n",
        "\n",
        "# You don't want the `Time` column.\n",
        "cleaned_df.pop('Time')\n",
        "\n",
        "# The `Amount` column covers a huge range. Convert to log-space.\n",
        "eps=0.001 # 0 => 0.1¢\n",
        "cleaned_df['Amount'] = np.log(cleaned_df['Amount'] + eps)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwX-E-z4OTx7"
      },
      "source": [
        "#questions a quoi sert values\n",
        "# Here, we split the data in (train, val, test) with (0.64, 0.16, 0.20),\n",
        "# i.e. test is 20% of all, train is 80% of the rest and val is 20% of the rest\n",
        "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
        "\n",
        "train_labels = train_df.pop('Class').values\n",
        "val_labels = val_df.pop('Class').values\n",
        "test_labels = test_df.pop('Class').values\n",
        "\n",
        "train_features = train_df.values\n",
        "val_features = val_df.values\n",
        "test_features = test_df.values"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDyYn7osPGFs",
        "outputId": "db7193b9-14e3-489d-dbdd-0c179573fb78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Fit a standard scaler to your training data and apply it to the test, val data\n",
        "# to transform you train data in 0-mean and 1-std\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        " \n",
        "val_features = scaler.transform(val_features)\n",
        "test_features = scaler.transform(test_features)\n",
        " \n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        " \n",
        " \n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)\n",
        " \n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training labels shape: (182276,)\n",
            "Validation labels shape: (45569,)\n",
            "Test labels shape: (56962,)\n",
            "Training features shape: (182276, 29)\n",
            "Validation features shape: (45569, 29)\n",
            "Test features shape: (56962, 29)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzKzks58xkyN"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Use the StandardScaler from sklearn to be more efficient and robust to outliers. Fit on the train data and apply train and test data.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "# hints:\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "\n",
        "val_features = scaler.transform(val_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "train_features = np.clip(train_features, -5, 5)\n",
        "val_features = np.clip(val_features, -5, 5)\n",
        "test_features = np.clip(test_features, -5, 5)\n",
        "\n",
        "\n",
        "print('Training labels shape:', train_labels.shape)\n",
        "print('Validation labels shape:', val_labels.shape)\n",
        "print('Test labels shape:', test_labels.shape)\n",
        "\n",
        "print('Training features shape:', train_features.shape)\n",
        "print('Validation features shape:', val_features.shape)\n",
        "print('Test features shape:', test_features.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5uvo9iDPbqv"
      },
      "source": [
        "### Checking features distributions\n",
        "\n",
        "Next compare the distributions of the positive and negative examples over a few features. Good questions to ask yourself at this point are:\n",
        "\n",
        "Do these distributions make sense?\n",
        "\n",
        "==> Yes. You've normalized the input and these are mostly concentrated in the +/- 2 range.\n",
        "\n",
        "Can you see the difference between the ditributions?\n",
        "\n",
        "==> Yes the positive examples contain a much higher rate of extreme values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iow8zlxIwZEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "outputId": "d92d1fd7-8d94-4808-e924-6a8a2a5484e2"
      },
      "source": [
        "bool_train_labels = (train_labels != 0)\n",
        "\n",
        "# We recreate a dataframe to plot the processed features and not the raw one\n",
        "pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns)\n",
        "neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns)\n",
        "\n",
        "sns.jointplot(pos_df['V5'], pos_df['V6'],\n",
        "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
        "\n",
        "plt.suptitle(\"Positive distribution\")\n",
        "\n",
        "sns.jointplot(neg_df['V5'], neg_df['V6'],\n",
        "              kind='hex', xlim = (-5,5), ylim = (-5,5))\n",
        "_ = plt.suptitle(\"Negative distribution\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGqCAYAAABeetDLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGqCAYAAABeetDLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQAEAskRAAQCyREABALL0fwK2zodlGk9EAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6AkytvdyA_K"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVpSUhD0LKY"
      },
      "source": [
        "### Defining Model & Metrics\n",
        "\n",
        "Notes about metrics:\n",
        "\n",
        "- False negatives and false positives are samples that were incorrectly classified\n",
        "\n",
        "- True negatives and true positives are samples that were correctly classified\n",
        "\n",
        "- Accuracy is the percentage of examples correctly classified:\n",
        "$\\frac{true \\, positive}{n_{samples}}$\n",
        "\n",
        "- Precision is the percentage of predicted positives that were correctly classified\n",
        "$\\frac{true \\, positive}{true \\, positive + false \\, positive}$\n",
        "\n",
        "- Recall is the percentage of actual positives that were correctly classified\n",
        "$\\frac{true \\, positive}{true \\, positive + false \\, negative}$\n",
        "\n",
        "- AUC refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1yYa5FC0N_R"
      },
      "source": [
        "# TODO: build a function build_model() to build a relevant model for this task\n",
        "# Here we ask you to pass to your model compiler a list of relevant metrics\n",
        "# to evaluate classification task, don't hesitate to include many of them,\n",
        "# it will help you to evaluate your model on this imbalanced dataset\n",
        "# check keras.metrics\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OiaihX6xsF_"
      },
      "source": [
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "\n",
        "def build_model(metrics = METRICS):\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid'),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djirZzVJ0bn6"
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AVgSWeM2T4f"
      },
      "source": [
        "# TODO: a good practice, after printing the summary of your model, is to\n",
        "# predict with your model before training it to make\n",
        "# sure it runs through and the output is correctly formatted\n",
        "# Predict on 10 input sample to check that"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO3hwoeXxtCZ"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model.predict(train_features[:10])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgTG6ga72snc"
      },
      "source": [
        "What do you think about the intial prediction done by the models? \n",
        "\n",
        "Are they similar to what you expect?\n",
        "\n",
        "Since we have a highly imbalanced dataset, we would rather see intial predictions very low, so we can change this before training our model so it's already clother to the truth and will converge faster\n",
        "\n",
        "see here for a reference of this trick, part \"init well\": [A Recipe for Training Neural Networks: \"init well\"](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h805ixO73QkP"
      },
      "source": [
        "# TODO: Implement the suggested trick, you just need to set the initial bias\n",
        "# and check on initial predictions it does what you expect\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEIdwdkDxugi"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "To set the bias you can use tf.keras.initializers.Constant\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "pos = bool_train_labels.sum()\n",
        "neg = (~bool_train_labels).sum()\n",
        "\n",
        "initial_bias = np.log([pos/neg])\n",
        "\n",
        "def build_model(metrics = METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "  model = keras.Sequential([\n",
        "      keras.layers.Dense(\n",
        "          16, activation='relu',\n",
        "          input_shape=(train_features.shape[-1],)),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(1, activation='sigmoid',\n",
        "                         bias_initializer=output_bias),\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "print(initial_bias)\n",
        "model = build_model(output_bias = initial_bias)\n",
        "model.predict(train_features[:10])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRTiZt8i3QoA"
      },
      "source": [
        "What do you observe? Is it closer to the expected value of $\\frac{pos}{total} = 0.0018$ ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHiATp3b1CPR"
      },
      "source": [
        "### Training: Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNdxWPTw4wLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bbd0d2f-050e-42be-c71e-a37f27b285ca"
      },
      "source": [
        "# Before training the model, we save the initial weights so we'll load them\n",
        "# for any training we do to compare the results\n",
        "\n",
        "# now commented so we don't save them again\n",
        "#initial_weights = os.path.join(tempfile.mkdtemp(),'initial_weights')\n",
        "print(initial_weights)\n",
        "#model.save_weights(initial_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tmp/tmpim9qcvog/initial_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUz5gu0Q1_8Z"
      },
      "source": [
        "# TODO: train your model with and without the biais trick and compare the results\n",
        "# (make sure the other weights are the same in your 2 experiments so it's faire to compare the results)\n",
        "# you can train for only 20 epochs at first to make it shorter and compare the two methods\n",
        "\n",
        "# NB: here you'll use a larger batch_size since you want to make sure there is\n",
        "# at least a few positive samples in each batch (try for example batch_size=2048)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmf3NlX4xypb"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to load your weights \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to load your weights \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "you can use early stopping \n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model.load_weights(initial_weights)\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "# ### CASE 1: without the biais\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "model.layers[-1].bias.assign([0.0])\n",
        "zero_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=1)\n",
        "\n",
        "# ### CASE 2: with the biais (it's already part of your saved weights)\n",
        "\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "careful_bias_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels), \n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFgR0vHAAzaM"
      },
      "source": [
        "# TODO: plot the evolution of the errors, we particularly care about the mae and mse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qz9YEsx0z8"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "zero_bias_history_df = pd.DataFrame(zero_bias_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "zero_bias_history_df.tail()\n",
        "\n",
        "careful_bias_history_df = pd.DataFrame(careful_bias_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "careful_bias_history_df.tail()\n",
        "\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  # plot zero bias\n",
        "  zero_bias_history_df.plot('epochs', f'{metric}', color='g', label='train zero bias', ax=ax)\n",
        "  zero_bias_history_df.plot('epochs', f'val_{metric}', color='r', label='validation zero bias', ax=ax)\n",
        "  # plot smart bias\n",
        "  careful_bias_history_df.plot('epochs', f'{metric}', color='g', label='train caregful bias', ax=ax, linestyle='--')\n",
        "  careful_bias_history_df.plot('epochs', f'val_{metric}', color='r', label='validation caregful bias', ax=ax, linestyle='--')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQXcdVuyBFZO"
      },
      "source": [
        "### Training: baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKf8L0i2CCiW"
      },
      "source": [
        "# TODO: train your model for 100 steps to set a baseline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVpkmYSpx-w5"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "model = build_model()\n",
        "model.load_weights(initial_weights)\n",
        "baseline_history = model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9sPnj_zCHk1"
      },
      "source": [
        "# TODO: plot the evolution of the errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRUpaygbx_k4"
      },
      "source": [
        "\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "\n",
        "baseline_history_df = pd.DataFrame(baseline_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "baseline_history_df.tail()\n",
        "\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train zero bias', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation zero bias', ax=ax)\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flopQ9hLCk8E"
      },
      "source": [
        "# TODO: evaluate your model on the test data (compute relevant metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxMptyPYyB-l"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "train_predictions_baseline = model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_baseline = model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "baseline_results = model.evaluate(test_features, test_labels,\n",
        "                                  batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(model.metrics_names, baseline_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZr3vJPqDAJe"
      },
      "source": [
        "# TODO: plot the confusion matrix, you can use a heatmap as a good way of displaying it\n",
        "# see here https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFT1V4EsyFgs"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "plot_cm(test_labels, test_predictions_baseline)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9OGdDR8DXQ2"
      },
      "source": [
        "Another relevant curve is the [ROC](https://developers.google.com/machine-learning/glossary#ROC) (Receiver operating characteristic) curve, it's the curve of true positive rate vs. false positive rate at different classification thresholds.\n",
        "\n",
        "The auc metric is the area under the curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hh8qlmJE3Sj"
      },
      "source": [
        "# TODO plot the roc curve\n",
        "# see scikit ref here https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
        "\n",
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,20])\n",
        "  plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oRMAZH8yPY0"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g',)\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, linestyle='--', color='r')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDqADt82FD3Q"
      },
      "source": [
        "Comment the curve:\n",
        "- Compare precision and recall values.\n",
        "- Think about which error your care the most: for instance here, a false negative (a fraudulent transaction is missed) may have a financial cost, while a false positive (a transaction is incorrectly flagged as fraudulent) may decrease user happiness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCDqefmOGEmD"
      },
      "source": [
        "### Training: class weights\n",
        "\n",
        "If there is a class which you want to emphasize in your training, you can heavily weight it when computing the loss. These will cause the model to \"pay more attention\" to examples from an under-represented class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zijwEPglGJqO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d01bfda0-c50e-4f37-d18f-586af2fa8703"
      },
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "neg = (raw_df.Class == 0).sum()\n",
        "pos = raw_df.Class.sum()\n",
        "total = len(raw_df)\n",
        "\n",
        "weight_for_0 = (1 / neg)*(total)/2.0 \n",
        "weight_for_1 = (1 / pos)*(total)/2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight for class 0: 0.50\n",
            "Weight for class 1: 289.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onfPxTbIG3L6"
      },
      "source": [
        "# TODO: train your model with the class weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByvvQ9tJyZWP"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "There is a nice arguments in the keras `.fit()` method called `class_weight`\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "weighted_model = build_model()\n",
        "weighted_model.load_weights(initial_weights)\n",
        "\n",
        "weighted_history = weighted_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels),\n",
        "    # The class weights go here\n",
        "    class_weight=class_weight) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1eASaczHJBa"
      },
      "source": [
        "# TODO: plot the evolution of the errorsn and evaluate your model on the test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1Anixlyaue"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "\n",
        "weighted_history_df = pd.DataFrame(weighted_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train baseline', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation baseline', ax=ax)\n",
        "\n",
        "  weighted_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='--')\n",
        "  weighted_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='--')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluation on the test set\n",
        "train_predictions_weighted = weighted_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_weighted = weighted_model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "weighted_results = weighted_model.evaluate(test_features, test_labels,\n",
        "                                           batch_size=BATCH_SIZE, verbose=0)\n",
        "for name, value in zip(weighted_model.metrics_names, weighted_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g')\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color='r')\n",
        "\n",
        "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color='g', linestyle='--')\n",
        "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color='r', linestyle='--')\n",
        "\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJmPEBQ1IvgY"
      },
      "source": [
        "Comment the curves and metrics:\n",
        "- How do the weights impact the different metrics?\n",
        "- Is this new model more relevant for your task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cjbNrlDJC3T"
      },
      "source": [
        "### Training: Oversampling\n",
        "\n",
        "Another popular approach to deal with highly imbalanced data is to modify the ratio of positive samples the model is seeing during the training phase. There are different ways of doing this, we can **oversample** the minority class or **undersample** the majority class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-OwjMZlJZ8m"
      },
      "source": [
        "# First we separate the pos and neg sample\n",
        "\n",
        "pos_features = train_features[bool_train_labels]\n",
        "neg_features = train_features[~bool_train_labels]\n",
        "\n",
        "pos_labels = train_labels[bool_train_labels]\n",
        "neg_labels = train_labels[~bool_train_labels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TuQnfYiK8-U"
      },
      "source": [
        "# TODO: use numpy random sampling method with replacement to oversample the positive classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb6O2NS5yftV"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "check the method np.random.choice\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Oversample to a positive ratio of 30% (if enough time you can try with other rates)\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Then use np.concatenate to reassemble your dataset and shuffle it\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>Hints\n",
        "</summary>\n",
        "Don't forget to update your initial bias and you change the number of samples so it might take longer to run an epoch\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "ids = np.arange(len(pos_features))\n",
        "oversampled_indices = np.random.choice(ids, int(0.3 * len(neg_features)), replace=True)\n",
        "\n",
        "res_pos_features = pos_features[oversampled_indices]\n",
        "res_pos_labels = pos_labels[oversampled_indices]\n",
        "\n",
        "oversampled_features = np.concatenate([res_pos_features, neg_features], axis=0)\n",
        "oversampled_labels = np.concatenate([res_pos_labels, neg_labels], axis=0)\n",
        "\n",
        "order = np.arange(len(oversampled_labels))\n",
        "np.random.shuffle(order)\n",
        "\n",
        "oversampled_features = oversampled_features[order]\n",
        "oversampled_labels = oversampled_labels[order]\n",
        "\n",
        "oversampled_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pQYZ5BrLn9J"
      },
      "source": [
        "# TODO: train your model for 100 epochs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFJvVzRl2s7x"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "oversampled_model = build_model()\n",
        "oversampled_model.load_weights(initial_weights)\n",
        "\n",
        "# Update the init biais\n",
        "pos = bool_train_labels.sum()\n",
        "neg = (~bool_train_labels).sum()\n",
        "\n",
        "oversampled_initial_bias = np.log([oversampled_labels.sum()/(oversampled_labels == 0).sum()])\n",
        "\n",
        "output_layer = oversampled_model.layers[-1] \n",
        "output_layer.bias.assign(oversampled_initial_bias)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).cache()\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(2) \n",
        "\n",
        "oversampled_history = oversampled_model.fit(\n",
        "    oversampled_features,\n",
        "    oversampled_labels,\n",
        "    epochs=100,\n",
        "    callbacks = [early_stopping],\n",
        "    validation_data=(val_features, val_labels))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcrZTkq1ODoM"
      },
      "source": [
        "# TODO: plot the evolution of the errors and compare it with previous methods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-O5ymx127H7"
      },
      "source": [
        "<details>\n",
        "<summary markdown='span'>View solution\n",
        "</summary>\n",
        "\n",
        "```python\n",
        "metrics_to_plot = ['loss', 'auc', 'precision', 'recall']\n",
        "\n",
        "oversampled_history_df = pd.DataFrame(oversampled_history.history).reset_index().rename(columns={'index': 'epochs'})\n",
        "\n",
        "fig, axes = plt.subplots(len(metrics_to_plot), 1, figsize=(14, 20))\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "  ax = axes[i]\n",
        "  baseline_history_df.plot('epochs', f'{metric}', color='g', label='train baseline', ax=ax)\n",
        "  baseline_history_df.plot('epochs', f'val_{metric}', color='r', label='validation baseline', ax=ax)\n",
        "\n",
        "  weighted_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='--')\n",
        "  weighted_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='--')\n",
        "\n",
        "  oversampled_history_df.plot('epochs', f'{metric}', color='g', label='train weighted', ax=ax, linestyle='dotted')\n",
        "  oversampled_history_df.plot('epochs', f'val_{metric}', color='r', label='validation weighted', ax=ax, linestyle='dotted')\n",
        "  ax.set_ylabel(metric)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate on test data and plot the ROC curve and compare it with previous methods\n",
        "\n",
        "train_predictions_oversampled = oversampled_model.predict(train_features, batch_size=BATCH_SIZE)\n",
        "test_predictions_oversampled = oversampled_model.predict(test_features, batch_size=BATCH_SIZE)\n",
        "\n",
        "oversampled_results = oversampled_model.evaluate(test_features, test_labels,\n",
        "                                             batch_size=BATCH_SIZE, verbose=0)\n",
        "\n",
        "for name, value in zip(oversampled_model.metrics_names, oversampled_results):\n",
        "  print(name, ': ', value)\n",
        "print()\n",
        "\n",
        "\n",
        "plot_roc(\"Train Baseline\", train_labels, train_predictions_baseline, color='g')\n",
        "plot_roc(\"Test Baseline\", test_labels, test_predictions_baseline, color='r')\n",
        "\n",
        "plot_roc(\"Train Weighted\", train_labels, train_predictions_weighted, color='g', linestyle='--')\n",
        "plot_roc(\"Test Weighted\", test_labels, test_predictions_weighted, color='r', linestyle='--')\n",
        "\n",
        "plot_roc(\"Train Oversampled\", train_labels, train_predictions_oversampled,  color='g', linestyle='dotted')\n",
        "plot_roc(\"Test Oversampled\", test_labels, test_predictions_oversampled,  color='r', linestyle='dotted')\n",
        "plt.legend(loc='lower right')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_ywEHEPvIh"
      },
      "source": [
        "Comment your results. You can also update the oversampling ratio and see how it affects your test metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKBz7DNlPm33"
      },
      "source": [
        "### Training: Undersampling\n",
        "\n",
        "Check out the effect of the undersampling strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBbpRibdPueA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03-Sentiment-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L4ncelot1024/Learn_Deep_Learning_Le_Wagon/blob/main/Day1/03_Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OO06xLD1oRz"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuW66mp_1oSB"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this exercise, you will do text classification starting from raw text (as\n",
        "a set of text files on disk). You will apply this workflow on the IMDB sentiment\n",
        "classification dataset (unprocessed version) where the goal is to say if a review is positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYWNNK2W1oSC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZCKZoke1oSD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK6D2DVpvAsm"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBIDWCBP8pvM"
      },
      "source": [
        "This first part is a good example of the beginning of any Deep Learning project: first we need to look at the data and prepare them ! \n",
        "\n",
        "Our data here are movie reviews written by users on the [imdb website](https://www.imdb.com/). We download them already labelled, the positive and negative ones are split in two different folders. \n",
        "\n",
        "But the data are not processed yet so we will use tensorflow to prepare the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPm5yxQY1oSD"
      },
      "source": [
        "### Load the data: IMDB movie review sentiment classification\n",
        "\n",
        "Let's download the data and inspect its structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsZMtVfD1oSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa375d83-ba74-4547-d5e7-48cc64eaf1b9"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  22.0M      0  0:00:03  0:00:03 --:--:-- 22.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A76ylcK61oSE"
      },
      "source": [
        "The `aclImdb` folder contains a `train` and `test` subfolder. The following bash commands prints the content of the different folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA_Q09Is1oSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "061090e4-c3ba-49a5-915d-094e6aa77f22"
      },
      "source": [
        "!ls aclImdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYxjwbnN1oSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42bcae9-9d2e-4c1e-f4c9-0b68ae2150c7"
      },
      "source": [
        "!ls aclImdb/test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD2ARRCm1oSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fff73e8-25bd-4fe5-fcf2-9776e1a415e5"
      },
      "source": [
        "!ls aclImdb/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh4osQqq1oSG"
      },
      "source": [
        "The `aclImdb/train/pos` and `aclImdb/train/neg` folders contain text files, each file represents one review (**positive** if in the *pos* folder or **negative** if in the *neg* folder). The following command display one example from each folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pQW3Phy1oSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feddb6a1-0f07-41c1-8ecc-c7112fefdef9"
      },
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv2W-0Ur9sq8",
        "outputId": "775f5179-b71b-4ac1-e34a-23b1eac4f63b"
      },
      "source": [
        "!cat aclImdb/test/neg/62_2.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Humm, an Italian movie starred by David hasselhoff and Linda Blair, I wasn´t expecting very much, to be honest and in fact, I took even less than I was expecting. It doesn´t mean this movie is the worst I have seen because I have watched worse things than this but the plot was most of the times confusing and uninteresting and some good gore scenes are the only thing saving this. Apart from that you are going to love some special effects, they are really cheesy and bad. Now I only want to watch \"Troll 3\" by this same director, sure it is not going to be worse than that."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6t56qxl1oSH"
      },
      "source": [
        "We are only interested in the `pos` and `neg` subfolders, so let's delete the rest:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Fe2rAFX1oSH"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cx54EN1oSH"
      },
      "source": [
        "Now we would like to load our date from the files into the memory so we can feed them into our model. \n",
        "\n",
        "To do that, use the utility `tf.keras.preprocessing.text_dataset_from_directory` to\n",
        "generate a labeled `tf.data.Dataset` object from a set of text files on disk filed into class-specific folders. \n",
        "\n",
        "We will also split the data since this function already contains the logic to split a dataset into validation and test (using the two argument `validation_split` and `subset`).\n",
        "\n",
        "Your goal here is to use this function to generate the training, validation, and test datasets. \n",
        "\n",
        "The validation and training datasets are generated from two subsets of the `train` directory, with the following ratio: 80% in the training and 20% in the validation. The test dataset comes directly from the test folder. \n",
        "\n",
        "You can use a reasonable batch size (for instance `32`). \n",
        "\n",
        "Finishes with a print of the sizes of your 3 sets. \n",
        "\n",
        "NB: When using the `validation_split` & `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation & training splits you get have no overlap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MFAtFrB3i49"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "raw_train_ds = ...\n",
        "raw_val_ds = ...\n",
        "raw_test_ds = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlwLy-P_3_8v"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "batch_size = 32\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=batch_size,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        ")\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Number of batches in raw_train_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_train_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",
        ")\n",
        "print(\n",
        "    \"Number of batches in raw_test_ds: %d\"\n",
        "    % tf.data.experimental.cardinality(raw_test_ds)\n",
        ")\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuUqlAKk1oSI"
      },
      "source": [
        "Let's preview a few samples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLxw0HlU1oSJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca429c1d-b10d-45bc-c3dc-fa99fcb97601"
      },
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"I gave this a four purely out of its historical context. It was considered lost for many years until it popped up out of the blue on Showtime in the early nineties.<br /><br />Moe is the straight man and Larry and Curly act as a duo. Spade Cooley has a couple of numbers. I guess it had something to do with working on a ranch. I'm not quite sure because the plot was so minimal nothing really sticks in my memory. I vaguely remember it being a western musical comedy. Even the Stooge's seem to be going through the motions. Overall there's nothing much really to recommend here.<br /><br />If you're not a Stooge fan then don't bother. If you are a Stooge fan, then stick with the shorts.\"\n",
            "0\n",
            "b'First than anything, I\\'m not going to praise I\\xc3\\xb1arritu\\'s short film, even I\\'m Mexican and proud of his success in mainstream Hollywood.<br /><br />In another hand, I see most of the reviews focuses on their favorite (and not so) short films; but we are forgetting that there is a subtle bottom line that circles the whole compilation, and maybe it will not be so pleasant for American people. (Even if that was not the main purpose of the producers) <br /><br />What i\\'m talking about is that most of the short films does not show the suffering that WASP people went through because the terrorist attack on September 11th, but the suffering of the Other people.<br /><br />Do you need proofs about what i\\'m saying? Look, in the Bosnia short film, the message is: \"You cry because of the people who died in the Towers, but we (The Others = East Europeans) are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you\".<br /><br />Even though the Burkina Fasso story is more in comedy, there is a the same thought: \"You are angry because Osama Bin Laden punched you in an evil way, but we (The Others = Africans) should be more angry, because our people is dying of hunger, poverty and AIDS long time ago, and nobody pay attention to us like the whole world has done to you\".<br /><br />Look now at the Sean Penn short: The fall of the Twin Towers makes happy to a lonely (and alienated) man. So the message is that the Power and the Greed (symbolized by the Towers) must fall for letting the people see the sun rise and the flowers blossom? It is remarkable that this terrible bottom line has been proposed by an American. There is so much irony in this short film that it is close to be subversive.<br /><br />Well, the Ken Loach (very know because his anti-capitalism ideology) is much more clearly and shameless in going straight to the point: \"You are angry because your country has been attacked by evil forces, but we (The Others = Latin Americans) suffered at a similar date something worst, and nobody remembers our grief as the whole world has done to you\".<br /><br />It is like if the creative of this project wanted to say to Americans: \"You see now, America? You are not the only that have become victim of the world violence, you are not alone in your pain and by the way, we (the Others = the Non Americans) have been suffering a lot more than you from long time ago; so, we are in solidarity with you in your pain... and by the way, we are sorry because you have had some taste of your own medicine\" Only the Mexican and the French short films showed some compassion and sympathy for American people; the others are like a slap on the face for the American State, that is not equal to American People.'\n",
            "1\n",
            "b'Blood Castle (aka Scream of the Demon Lover, Altar of Blood, Ivanna--the best, but least exploitation cinema-sounding title, and so on) is a very traditional Gothic Romance film. That means that it has big, creepy castles, a headstrong young woman, a mysterious older man, hints of horror and the supernatural, and romance elements in the contemporary sense of that genre term. It also means that it is very deliberately paced, and that the film will work best for horror mavens who are big fans of understatement. If you love films like Robert Wise\\'s The Haunting (1963), but you also have a taste for late 1960s/early 1970s Spanish and Italian horror, you may love Blood Castle, as well.<br /><br />Baron Janos Dalmar (Carlos Quiney) lives in a large castle on the outskirts of a traditional, unspecified European village. The locals fear him because legend has it that whenever he beds a woman, she soon after ends up dead--the consensus is that he sets his ferocious dogs on them. This is quite a problem because the Baron has a very healthy appetite for women. At the beginning of the film, yet another woman has turned up dead and mutilated.<br /><br />Meanwhile, Dr. Ivanna Rakowsky (Erna Sch\\xc3\\xbcrer) has appeared in the center of the village, asking to be taken to Baron Dalmar\\'s castle. She\\'s an out-of-towner who has been hired by the Baron for her expertise in chemistry. Of course, no one wants to go near the castle. Finally, Ivanna finds a shady individual (who becomes even shadier) to take her. Once there, an odd woman who lives in the castle, Olga (Cristiana Galloni), rejects Ivanna and says that she shouldn\\'t be there since she\\'s a woman. Baron Dalmar vacillates over whether she should stay. She ends up staying, but somewhat reluctantly. The Baron has hired her to try to reverse the effects of severe burns, which the Baron\\'s brother, Igor, is suffering from.<br /><br />Unfortunately, the Baron\\'s brother appears to be just a lump of decomposing flesh in a vat of bizarre, blackish liquid. And furthermore, Ivanna is having bizarre, hallucinatory dreams. Just what is going on at the castle? Is the Baron responsible for the crimes? Is he insane? <br /><br />I wanted to like Blood Castle more than I did. As I mentioned, the film is very deliberate in its pacing, and most of it is very understated. I can go either way on material like that. I don\\'t care for The Haunting (yes, I\\'m in a very small minority there), but I\\'m a big fan of 1960s and 1970s European horror. One of my favorite directors is Mario Bava. I also love Dario Argento\\'s work from that period. But occasionally, Blood Castle moved a bit too slow for me at times. There are large chunks that amount to scenes of not very exciting talking alternated with scenes of Ivanna slowly walking the corridors of the castle.<br /><br />But the atmosphere of the film is decent. Director Jos\\xc3\\xa9 Luis Merino managed more than passable sets and locations, and they\\'re shot fairly well by Emanuele Di Cola. However, Blood Castle feels relatively low budget, and this is a Roger Corman-produced film, after all (which usually means a low-budget, though often surprisingly high quality \"quickie\"). So while there is a hint of the lushness of Bava\\'s colors and complex set decoration, everything is much more minimalist. Of course, it doesn\\'t help that the Retromedia print I watched looks like a 30-year old photograph that\\'s been left out in the sun too long. It appears \"washed out\", with compromised contrast.<br /><br />Still, Merino and Di Cola occasionally set up fantastic visuals. For example, a scene of Ivanna walking in a darkened hallway that\\'s shot from an exaggerated angle, and where an important plot element is revealed through shadows on a wall only. There are also a couple Ingmar Bergmanesque shots, where actors are exquisitely blocked to imply complex relationships, besides just being visually attractive and pulling your eye deep into the frame.<br /><br />The performances are fairly good, and the women--especially Sch\\xc3\\xbcrer--are very attractive. Merino exploits this fact by incorporating a decent amount of nudity. Sch\\xc3\\xbcrer went on to do a number of films that were as much soft corn porn as they were other genres, with English titles such as Sex Life in a Woman\\'s Prison (1974), Naked and Lustful (1974), Strip Nude for Your Killer (1975) and Erotic Exploits of a Sexy Seducer (1977). Blood Castle is much tamer, but in addition to the nudity, there are still mild scenes suggesting rape and bondage, and of course the scenes mixing sex and death.<br /><br />The primary attraction here, though, is probably the story, which is much a slow-burning romance as anything else. The horror elements, the mystery elements, and a somewhat unexpected twist near the end are bonuses, but in the end, Blood Castle is a love story, about a couple overcoming various difficulties and antagonisms (often with physical threats or harms) to be together.'\n",
            "1\n",
            "b\"Halloween is one of the best examples of independent film. It's very well made and has more psychological elements to it than you might realize at first glance. It is a simple movie told very well. The music is perfect and is one of the most haunting scores... If you haven't seen this movie yet, you must check it out. The cast is all terrific. I wish they had never made sequel after sequel. The first one was by far the best and should have ended like it did without having a sequel. It was fun to see Jamie Lee Curtis in the movie. She hasn't seemed to age (she's just as gorgeous today, without the hairdo and seventies clothes). The scenes through the mask are one of the scariest things ever!\"\n",
            "1\n",
            "b\"Michelle Rodriguez is the defining actress who could be the charging force for other actresses to look out for. She has the audacity to place herself in a rarely seen tough-girl role very early in her career (and pull it off), which is a feat that should be recognized. Although her later films pigeonhole her to that same role, this film was made for her ruggedness.<br /><br />Her character is a romanticized student/fighter/lover, struggling to overcome her disenchanted existence in the projects, which is a little overdone in film...but not by a girl. That aspect of this film isn't very original, but the story goes in depth when the heated relationships that this girl has to deal with come to a boil and her primal rage takes over.<br /><br />I haven't seen an actress take such an aggressive stance in movie-making yet, and I'm glad that she's getting that original twist out there in Hollywood. This film got a 7 from me because of the average story of ghetto youth, but it has such a great actress portraying a rarely-seen role in a minimal budget movie. Great work.\"\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yt90kfX_bSi"
      },
      "source": [
        "Important: Your dataset is not prepared yet, you just explained how to load it !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3PxW6I71oSJ"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "From above, you see there are some noise in the data, for instance the `<br />` tags. So we want to remove them !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsrTEWZl42dw"
      },
      "source": [
        "Having looked at our data above, we see that the raw text contains HTML break\n",
        "tags of the form `<br />`. \n",
        "\n",
        "These tags will not be removed by the default standardizer (which doesn't strip HTML). Because of this, we will need to create a custom standardization function.\n",
        "\n",
        "Write a function `custom_standardisation` which take text as input, removes the tags of the form  `<br />` and proceeds to tradional standardisation: lowering caracters, removing punctuation. \n",
        "\n",
        "We recommend using the internal tensorflow methods inside `tf.strings`: `tf.strings.lower`, `tf.strings.regex_replace`, `string/punctuation`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsPY1TX2590M"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "  # YOUR CODE HERE\n",
        "  return input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbz59ACr5ul-"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
        "    )\n",
        "```\n",
        "</details>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu4rOFoN6L1E"
      },
      "source": [
        "Now we will use this function in a vectoriation workflow using `TextVectorization`. This helper will normalize, split, and map strings to integers. \n",
        "\n",
        "We suggest default values for the model constants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCLnGpak4kHS"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "# Model constants.\n",
        "max_features = 20000\n",
        "sequence_length = 80\n",
        "\n",
        "# YOUR CODE HERE\n",
        "vectorize_layer = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj3lioWq6-P1"
      },
      "source": [
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "    \n",
        "```markdown\n",
        "Have  loot at the documentation [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization)\n",
        "```\n",
        "    \n",
        "</details>\n",
        "​\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRgOFrhM7Lig"
      },
      "source": [
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Sh0sif1oSK"
      },
      "source": [
        "### Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:\n",
        "\n",
        "**Option 1: Make it part of the model**, so as to obtain a model that processes raw\n",
        " strings, like this:\n",
        "\n",
        "```python\n",
        "text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\n",
        "x = vectorize_layer(text_input)\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0PMhGOE1oSK"
      },
      "source": [
        "**Option 2: Apply it to the text dataset** to obtain a dataset of word indices, then\n",
        " feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two methods is that  option 2 enables you to do\n",
        "**asynchronous CPU processing and buffering** of your data when training your model on the graphical processing unit, a.k.a GPU (this is the hardware on which a neural network is trained). This is the case since you run first your pre-processing on all the data and then train your model, so the first step can occur on the CPU, where it will be more efficient.\n",
        "\n",
        "\n",
        "Here since we're training the model on GPU, we want to go with this 2nd option to get the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw\n",
        "strings as input so we would use the first option. But we can actually recover this first option from a model trained with the second one. We do this in the last section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed1qtC_bCN5f"
      },
      "source": [
        "Write a function you will apply to your 3 datasets (`raw_train_ds`, `raw_val_ds` and `raw_test_ds`) to vectorize them. Recall that each dataset contains tuple of `(text, label)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuTQBZqUCvU0"
      },
      "source": [
        "# This function should return 2 elements: the text vectorized and the label\n",
        "def vectorize_text(text, label):\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kImaPJuXCzef"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2wmmYEjCIAo"
      },
      "source": [
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGFyHuA_Lqtc",
        "outputId": "4e4f5fa3-3ca2-4b76-9a97-0aef547df97f"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "      print(text_batch.numpy()[0])\n",
        "      print(label_batch.numpy()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  100     2     1     3  5111  6110   303     2  2340  1855 17105     8\n",
            "     2   783     5  5161    11   179    72  2135   145  2340  1855  2442\n",
            "    30    12   209   234   143    67     2    60   106    12   801    69\n",
            " 14710     8     3   234  1050     2   144 19022   852     2     1   328\n",
            "   452    52  2343     6    69    11  5450     3   632  2747     8    20\n",
            "  2340     1    11    60  3775     6  2340  1855   244     8  5954    15\n",
            "    12    28   339    60    15    28     3    60]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK0anF0uvHG4"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEvH3JZ1oSL"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "First, we choose a very simple architecture for our baseline model:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- 1 `LSTM` layer;\n",
        "- 1 `Dense` layer for the final prediction.\n",
        "\n",
        "We will use the `adam` optimizer. The task is to classify our entries between two classes, what should be the size of the final `Dense` layer and which loss will you use?\n",
        "\n",
        "Then you can try out more advanced architectures at the end!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiQIFqeXEJ8F"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "\n",
        "\n",
        "def build_baseline_model(embedding_dim = 128):\n",
        "  model = Sequential()\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  # add your layers\n",
        "  model.add(...)\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  # Compile the model \n",
        "  model.compile(...)\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65VRrmmazW43"
      },
      "source": [
        "\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def build_baseline_model(embedding_dim=32, conv_dim=128, num_filters=7):\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(LSTM(128, dropout=0.2))\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F59JWbAO1oSL"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rceSKY131oSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e31905-785c-4518-f815-d3bc7a8db625"
      },
      "source": [
        "epochs = 9\n",
        "\n",
        "model = build_baseline_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 4s 18ms/step - loss: 0.6509 - accuracy: 0.5793 - val_loss: 0.4306 - val_accuracy: 0.8114\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.3727 - accuracy: 0.8424 - val_loss: 0.4238 - val_accuracy: 0.8228\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.2613 - accuracy: 0.9009 - val_loss: 0.4937 - val_accuracy: 0.8200\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.2005 - accuracy: 0.9321 - val_loss: 0.5870 - val_accuracy: 0.7874\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 2s 14ms/step - loss: 0.1791 - accuracy: 0.9381 - val_loss: 0.5050 - val_accuracy: 0.8028\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.1688 - accuracy: 0.9405 - val_loss: 0.7473 - val_accuracy: 0.7860\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.1672 - accuracy: 0.9372 - val_loss: 0.5483 - val_accuracy: 0.7908\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.1233 - accuracy: 0.9589 - val_loss: 0.6298 - val_accuracy: 0.7980\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 2s 15ms/step - loss: 0.0989 - accuracy: 0.9667 - val_loss: 0.7966 - val_accuracy: 0.7824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HjuY0b91oSM"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEeKDGk3vnSO"
      },
      "source": [
        "def plot_history(history):\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "FUmksMuIqhNg",
        "outputId": "9ffd0f9e-7bbd-46d4-ba66-7ea2fa714ff9"
      },
      "source": [
        "# Plot history\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld6eBpRTv6vr"
      },
      "source": [
        "Apply your model on your test dataset and evaluate its predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_enDofC1oSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1274863-148c-44ca-9a3e-05d380829000"
      },
      "source": [
        "evaluations = model.evaluate(test_ds)\n",
        "test_evaluations = {name: value for value, name in zip(evaluations, model.metrics_names)}\n",
        "print(test_evaluations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 1s 5ms/step - loss: 0.9089 - accuracy: 0.7532\n",
            "{'loss': 0.9089375138282776, 'accuracy': 0.7531999945640564}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4WFibkSv_zH"
      },
      "source": [
        "<details>\n",
        "  <summary>Hint</summary>\n",
        "    \n",
        "```markdown\n",
        "Check the `model.evaluate` function documentation (how could you retrieve the name of the float returned?)\n",
        "```\n",
        "    \n",
        "</details>\n",
        "​\n",
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "evaluations = model.evaluate(test_ds)\n",
        "test_evaluations = {name: value for value, name in zip(evaluations, model.metrics_names)}\n",
        "print(test_evaluations)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrdU5KvF1oSO"
      },
      "source": [
        "## Make an end-to-end model\n",
        "\n",
        "If you want to obtain a model capable of processing raw strings, you can simply\n",
        "create a new model, using the baseline you just defined and adding as a first step the vectorize layer. You can again do that with the Keras `Sequential` API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GLnYJsW1oSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968d144a-a671-4369-de21-b9a3d0bfdd45"
      },
      "source": [
        "end2end_model = ...\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196/196 [==============================] - 9s 41ms/step - loss: 0.9130 - accuracy: 0.7528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9089372754096985, 0.7531999945640564]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwYyBRVyw9ld"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "end2end_model = tf.keras.models.Sequential()\n",
        "# A string input\n",
        "end2end_model.add(tf.keras.Input(shape=(1,), dtype=\"string\"))\n",
        "# Turn strings into vocab indices\n",
        "end2end_model.add(vectorize_layer)\n",
        "end2end_model.add(model)\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "end_to_end_model.evaluate(raw_test_ds)\n",
        "\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox8sDbBCxJ-P"
      },
      "source": [
        "You can now apply this model on any input, so give it a try !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVI0UB2frJTE"
      },
      "source": [
        "# Create your own input\n",
        "my_input_good = \"I liked very much this movie\"\n",
        "my_input_good_2 = \"This movie is not very bad though.\"\n",
        "my_input_good_3 = \"I fall asleep\"\n",
        "my_input_bad = \"This movie is terribly bad\"\n",
        "\n",
        "# You just need to convert your string into a tensor\n",
        "# YOUR CODE HERE\n",
        "text_input = ...\n",
        "end_to_end_model.predict(text_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxBZRJCcxSjp"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "text_input = tf.convert_to_tensor([my_input_good, my_input_good_2, my_input_good_3, my_input_bad], dtype=tf.string)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Hbt43LFLPz"
      },
      "source": [
        "## Try out more complex architectures !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWX9w89du4VD"
      },
      "source": [
        "### Bi-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4bAEdDyMJs"
      },
      "source": [
        "Here we suggest the following architecture:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- 1 or several `Bidirectional` layers (if multiple you need the first one to be a many-to-many one !)\n",
        "- 1 Dense layer to add more complexity;\n",
        "- 1 Dense layer for the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpO7DkMyLee"
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "def build_bi_lstm_model(embedding_dim=128, dense_dim=128):\n",
        "  # YOUR CODE HERE\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1yc6fgPytSS"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "def build_bi_lstm_model(embedding_dim=128, dense_dim=128):\n",
        "  # A integer input for vocab indices.\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Conv1D + global max pooling\n",
        "  model.add(Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "  model.add(Bidirectional(layers.LSTM(64)))\n",
        "\n",
        "  # We add a vanilla hidden layer:\n",
        "  model.add(Dense(dense_dim, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "  model.add(Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99W-_t4_ymEF",
        "outputId": "99c139f4-115d-4139-ad20-372e5f62cc3b"
      },
      "source": [
        "epochs = 9\n",
        "\n",
        "bi_lstm_model = build_bi_lstm_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "bi_lstm_history = bi_lstm_model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 14s 57ms/step - loss: 0.6162 - accuracy: 0.6141 - val_loss: 0.3937 - val_accuracy: 0.8246\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.3526 - accuracy: 0.8475 - val_loss: 0.4185 - val_accuracy: 0.8196\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.2469 - accuracy: 0.9029 - val_loss: 0.4731 - val_accuracy: 0.8054\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.1725 - accuracy: 0.9358 - val_loss: 0.5011 - val_accuracy: 0.8052\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.1241 - accuracy: 0.9548 - val_loss: 0.7450 - val_accuracy: 0.7734\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0977 - accuracy: 0.9640 - val_loss: 0.7192 - val_accuracy: 0.8006\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.0710 - accuracy: 0.9740 - val_loss: 0.8597 - val_accuracy: 0.8040\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0656 - accuracy: 0.9768 - val_loss: 1.0517 - val_accuracy: 0.7664\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0560 - accuracy: 0.9810 - val_loss: 1.1177 - val_accuracy: 0.7858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLQAdsm9u73Y"
      },
      "source": [
        "### 1D-Convnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csnUisqWu68g"
      },
      "source": [
        "Here we suggest the following architecture:\n",
        "- `Embedding` layer to create embedding for our entries;\n",
        "- eventually a Dropout after for regularization;\n",
        "- 2 layers 1D `Conv1D`\n",
        "- 1 GlobalMaxPooling1D layer to pool;\n",
        "- 1 Dense layer for the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mqN3y3h1oSL"
      },
      "source": [
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "def build_cnn_model(embedding_dim=128, conv_dim=128, num_filters=7):\n",
        "  # YOUR CODE HERE\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRdHWfy9y1wN"
      },
      "source": [
        "<details>\n",
        "  <summary>View solution</summary>\n",
        "    \n",
        "```python\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "\n",
        "def build_cnn_model(embedding_dim=128, conv_dim=128, num_filters=7):\n",
        "  # A integer input for vocab indices.\n",
        "  model = Sequential()\n",
        "\n",
        "  # Next, we add a layer to map those vocab indices into a space of dimensionality\n",
        "  # 'embedding_dim'.\n",
        "  model.add(Embedding(max_features, embedding_dim))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Conv1D + global max pooling\n",
        "  model.add(Conv1D(conv_dim, num_filters, padding=\"valid\", activation=\"relu\", strides=3))\n",
        "  model.add(Conv1D(conv_dim, num_filters, padding=\"valid\", activation=\"relu\", strides=3))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "\n",
        "  # We add a vanilla hidden layer:\n",
        "  model.add(Dense(conv_dim, activation=\"relu\"))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "  model.add(Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n",
        "\n",
        "  # Compile the model with binary crossentropy loss and an adam optimizer.\n",
        "  model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iklP87wtFOEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e099731-8278-49c5-d404-e2bfd4da8327"
      },
      "source": [
        "epochs = 9\n",
        "\n",
        "cnn_model = build_cnn_model()\n",
        "# Fit the model using the train and test datasets.\n",
        "cnn_history = cnn_model.fit(train_ds, validation_data=val_ds, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/9\n",
            "157/157 [==============================] - 6s 32ms/step - loss: 0.6861 - accuracy: 0.5211 - val_loss: 0.4645 - val_accuracy: 0.7806\n",
            "Epoch 2/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.4473 - accuracy: 0.7964 - val_loss: 0.4323 - val_accuracy: 0.8102\n",
            "Epoch 3/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.2618 - accuracy: 0.8996 - val_loss: 0.5236 - val_accuracy: 0.7990\n",
            "Epoch 4/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.1451 - accuracy: 0.9495 - val_loss: 0.6559 - val_accuracy: 0.7948\n",
            "Epoch 5/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0952 - accuracy: 0.9660 - val_loss: 0.8134 - val_accuracy: 0.7830\n",
            "Epoch 6/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0773 - accuracy: 0.9716 - val_loss: 0.8962 - val_accuracy: 0.7988\n",
            "Epoch 7/9\n",
            "157/157 [==============================] - 5s 31ms/step - loss: 0.0497 - accuracy: 0.9830 - val_loss: 1.0438 - val_accuracy: 0.7940\n",
            "Epoch 8/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0206 - accuracy: 0.9924 - val_loss: 1.1188 - val_accuracy: 0.7956\n",
            "Epoch 9/9\n",
            "157/157 [==============================] - 5s 30ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 1.1047 - val_accuracy: 0.7960\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}